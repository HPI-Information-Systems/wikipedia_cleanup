{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import Levenshtein\n",
    "import matplotlib.pyplot as plt\n",
    "import fastDamerauLevenshtein"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# import os\n",
    "# import random\n",
    "# rnd_files = random.sample(os.listdir(\"../../../matched-infoboxes/\"), 20)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# from shutil import copyfile\n",
    "# for fil in rnd_files:\n",
    "#     copyfile(\"../../../matched-infoboxes/\"+fil, \"../../../batch/\"+fil)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### How does it work\n",
    "Typos:\n",
    "- previous and current values are splittet into words with non letters/whitespaces removed, if any are None or empty they are skipped\n",
    "- only same wordcounts are tested (it is expected that words stay in the same order)\n",
    "- words are compared to the words with the same index with Damerau-Levenshtein edit distance (swaps are cost 1 not 2)\n",
    "- if any word has edit distance 1 or 2 it is further looked at\n",
    "    1. test if first letter is a case swap (it is expected the user knows what is correct)\n",
    "    2. test if previous word is not in a dictionary but current word is (comparison is done in lowercase)\n",
    "- if any of the tests is true the change is marked as typo-fix\n",
    "\n",
    "Swear words:\n",
    "- previous and current values are splittet into words, if any are None or empty they are skipped\n",
    "- words are compared to a swear word dictionary (https://github.com/RobertJGabriel/Google-profanity-words/blob/master/list.txt)\n",
    "- if any word is matched the value is flagged as swear word\n",
    "- if previous value has no swear word but current value has -> swear word added\n",
    "- if previous value has a swear word but current value has not -> swear word deleted"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# splits string in words\n",
    "def split_strings(str1, str2):\n",
    "    lst = [str1.split()]\n",
    "    lst.append(str2.split())\n",
    "    return lst\n",
    "\n",
    "# checks if wordcount in both strings is equal\n",
    "\n",
    "\n",
    "def same_wordcounts(lst1, lst2):\n",
    "    return (len(lst1) == len(lst2))\n",
    "\n",
    "# deletes non alphabetical characters from string\n",
    "\n",
    "\n",
    "def skip_no_alpha(string):\n",
    "    only_alpha = \"\"\n",
    "    for char in string:\n",
    "        if char.isalpha() or char == \" \":\n",
    "            only_alpha += char\n",
    "    return only_alpha\n",
    "\n",
    "# checks in numbers are increments\n",
    "\n",
    "\n",
    "def is_increment(nr1, nr2):\n",
    "    return (nr1+1 == nr2 or nr1-1 == nr2)\n",
    "\n",
    "# checks if case (upper/loewr) of the first latter is switched\n",
    "\n",
    "\n",
    "def is_first_letter_caseswitch(str1, str2):\n",
    "    return (str1[0].isupper() and str2[0].islower() or str1[0].islower() and str2[0].isupper())\n",
    "\n",
    "\n",
    "def is_not_empty_or_none(input):\n",
    "    return input is not None and input is not \"\"\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "def get_levenshtein_dists(lst1, lst2):\n",
    "    if len(lst1) != len(lst2):\n",
    "        print(\"Difference words counts of lists!\")\n",
    "        return\n",
    "    dists = []\n",
    "    for i in range(len(lst1)):\n",
    "            dists.append(int(fastDamerauLevenshtein.damerauLevenshtein(\n",
    "                lst1[i], lst2[i], similarity=False)))\n",
    "    return dists\n",
    "\n",
    "# splits strings in words\n",
    "def get_words_and_dists(str1, str2, only_alpha=False):\n",
    "    if only_alpha:\n",
    "        str1=skip_no_alpha(str1)\n",
    "        str2=skip_no_alpha(str2)\n",
    "    words = split_strings(str1, str2)\n",
    "    if len(words[0]) == len(words[1]):\n",
    "        dists = get_levenshtein_dists(words[0], words[1])\n",
    "    else:\n",
    "        dists = []\n",
    "    return words, dists\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "def word_in_dict(str1, words_dict):\n",
    "    return str1 in words_dict\n",
    "\n",
    "def is_typo_fixed(str1, str2, words_dict, lowercase=True):\n",
    "    \"\"\" Check if typo is fixed.\n",
    "        return 0: no other case is found\n",
    "        return 1: word was not in dict before (missspelled)\n",
    "        return 2: word with swapped first letter (and other changes depending on edit distance)\n",
    "    \"\"\"\n",
    "    # detects number errors (dreher,tippfehler), skipps increments. Only works if skip_no_alpha is false \n",
    "    if str1.isdigit() and str2.isdigit() and not is_increment(int(str1),int(str2)):\n",
    "        return 3\n",
    "\n",
    "    if is_first_letter_caseswitch(str1,str2):\n",
    "        return 2\n",
    "\n",
    "    if lowercase:\n",
    "        str1=str1.lower()\n",
    "        str2=str2.lower()\n",
    "\n",
    "    # checks if str1 is not in dict but str2 is\n",
    "    if (not word_in_dict(str1, words_dict) and word_in_dict(str2, words_dict)):\n",
    "        return 1\n",
    "        \n",
    "    return 0"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "def get_typo_type(str1, str2, words_dict, upper_lev_distance=2, skip_no_alpha=False):\n",
    "    words, levenshtein_dists = get_words_and_dists(\n",
    "        str1, str2, skip_no_alpha)\n",
    "    typo_lst = []\n",
    "    for i in range(len(levenshtein_dists)):  # only loops if dists are found (word counts are equal)\n",
    "        # only uses distances >0 <=2\n",
    "        if(levenshtein_dists[i] > 0 and levenshtein_dists[i] <= upper_lev_distance):\n",
    "            typo_lst.append(is_typo_fixed(\n",
    "                words[0][i], words[1][i], words_dict))\n",
    "        # else:  # appends None if dist is <0 or >2\n",
    "        #     typo_lst.append(None)\n",
    "    return typo_lst\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "my_file = open(\"../../../words_alpha.txt\", \"r\")\n",
    "words_dict=set(my_file.read().split(\"\\n\"))\n",
    "\n",
    "testcase1 = [\"Hier sind kkeine Fheler\", \"Hier sind keine Fehler\"]\n",
    "testcase1_en = [\"There are nno erorrs\", \"There are no errors\"]\n",
    "typo_lst = get_typo_type(testcase1_en[0], testcase1_en[1], words_dict)\n",
    "print(typo_lst)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[1, 1]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "def typo_check(str1, str2, words_dict, upper_lev_distance=2, skip_no_alpha=False):\n",
    "    \"\"\"Return True if typo\n",
    "       Return False if no typo\n",
    "    \"\"\"\n",
    "    typo_lst = get_typo_type(str1, str2, words_dict, upper_lev_distance, skip_no_alpha)\n",
    "    if len(typo_lst) == 0:\n",
    "        return None\n",
    "    for typo_type in typo_lst:\n",
    "        if typo_type > 0:  # 1 is previous not in dict, current is in dict, 2 case switch on first letter\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def typo_check_pandas(row, words_dict, upper_lev_distance=2, skip_no_alpha=False):\n",
    "    \"\"\"Return True if typo\n",
    "       Return False if no typo\n",
    "    \"\"\"\n",
    "    typo_lst = get_typo_type(row[\"currentValue\"], row[\"previousValue\"], words_dict, upper_lev_distance, skip_no_alpha)\n",
    "    if len(typo_lst) == 0:\n",
    "        return None\n",
    "    for typo_type in typo_lst:\n",
    "        if typo_type > 0:  # 1 is previous not in dict, current is in dict, 2 case switch on first letter\n",
    "            return True\n",
    "    return False"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "source": [
    "data"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [pageTitle, pageID, key, template, name, previousValue, currentValue, validFrom, validTo, revisionId, user_name, user_id, user_ip, attributes]\n",
       "Index: []"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pageTitle</th>\n",
       "      <th>pageID</th>\n",
       "      <th>key</th>\n",
       "      <th>template</th>\n",
       "      <th>name</th>\n",
       "      <th>previousValue</th>\n",
       "      <th>currentValue</th>\n",
       "      <th>validFrom</th>\n",
       "      <th>validTo</th>\n",
       "      <th>revisionId</th>\n",
       "      <th>user_name</th>\n",
       "      <th>user_id</th>\n",
       "      <th>user_ip</th>\n",
       "      <th>attributes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 54
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "data[\"isTypo\"]=data.apply(lambda row: typo_check_pandas(\n",
    "    row, words_dict, upper_lev_distance=2, skip_no_alpha=False),axis=1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "def count_typos(typo_lst):\n",
    "    counts = {\"typo fixed\": 0,\n",
    "              \"no typo\": 0,\n",
    "              \"no levenshstein match\": 0,\n",
    "              \"not tested\": 0\n",
    "              }\n",
    "    for typo in typo_lst:\n",
    "        if typo is True:\n",
    "            counts[\"typo fixed\"] += 1\n",
    "        if typo is False:\n",
    "            counts[\"no typo\"] += 1\n",
    "        if typo is None:\n",
    "            counts[\"no levenshstein match\"] += 1\n",
    "        if typo is \"not_tested\":\n",
    "            counts[\"not tested\"] += 1\n",
    "    counts[\"tested\"] = counts[\"typo fixed\"] + \\\n",
    "        counts[\"no typo\"]+counts[\"no levenshstein match\"]\n",
    "    print(\"typo fixed:\", counts[\"typo fixed\"])\n",
    "    print(\"no typo:\", counts[\"no typo\"])\n",
    "    print(\"no levenshstein match:\", counts[\"no levenshstein match\"])\n",
    "    print(\"not tested:\", counts[\"not tested\"])\n",
    "    print(\"total changes:\", counts[\"typo fixed\"]+counts[\"no typo\"] +\n",
    "          counts[\"not tested\"]+counts[\"no levenshstein match\"])\n",
    "    print(\"% of matching levenshtein distance:\",\n",
    "          (counts[\"typo fixed\"]+counts[\"no typo\"])/counts[\"tested\"]*100)\n",
    "    print(\"typo fix % of tested changes (only updates without creations/deletions):\",\n",
    "          counts[\"typo fixed\"]/counts[\"tested\"]*100)\n",
    "    print(\"typo fix % of all changes:\",\n",
    "          (counts[\"typo fixed\"]/(counts[\"typo fixed\"]+counts[\"no typo\"]+counts[\"not tested\"]))*100)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "input_data = Path(\"../../matched-infoboxes-extracted/\")\n",
    "inp = list(input_data.rglob('*.json'))\n",
    "files = [x for x in inp if x.is_file()]\n",
    "print(\"number of files:\", len(files))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "number of files: 20\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "source": [
    "fileshare_path=Path(\"Z:\\mp2021\\mpws2021\\MPWS2021FN1\")\n",
    "my_file = open(fileshare_path / \", \"r\")\n",
    "words_dict=set(my_file.read().split(\"\\n\"))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "num_iteratons = 1\n",
    "number_of_files = 1\n",
    "number_of_files_start = 10\n",
    "num_edits = 0\n",
    "num_change_tuples = 0\n",
    "typo_lst = []\n",
    "timedeltas_all = []\n",
    "timedeltas_typo = []\n",
    "timedeltas_levensh = []\n",
    "for _ in range(num_iteratons):\n",
    "    change_tuples = []\n",
    "    for file in tqdm(files[number_of_files_start:number_of_files_start+number_of_files]):\n",
    "        with open(file, 'r', encoding='utf-8') as f:\n",
    "            for jsonObj in f:\n",
    "                single_edit = json.loads(jsonObj)\n",
    "                num_edits += 1\n",
    "                title = single_edit['pageTitle']\n",
    "                pageID = single_edit['pageID']\n",
    "                key = single_edit['key']\n",
    "                template = single_edit['template'] if 'template' in single_edit.keys(\n",
    "                ) else None\n",
    "                changes = single_edit['changes']\n",
    "                validFrom = single_edit['validFrom']\n",
    "                revisionId = single_edit['revisionId']\n",
    "                attributes = single_edit['attributes'] if 'attributes' in single_edit.keys(\n",
    "                ) else None\n",
    "                user_name = single_edit['user']['username'] if 'username' in single_edit['user'].keys(\n",
    "                ) else None\n",
    "                user_id = single_edit['user']['id'] if 'id' in single_edit['user'].keys(\n",
    "                ) else None\n",
    "                user_ip = single_edit['user']['ip'] if 'ip' in single_edit['user'].keys(\n",
    "                ) else None\n",
    "                for change in changes:\n",
    "                    name = change['property']['name']\n",
    "                    current_value = change['currentValue'] if 'currentValue' in change.keys(\n",
    "                    ) else None\n",
    "                    previous_value = change['previousValue'] if 'previousValue' in change.keys(\n",
    "                    ) else None\n",
    "                    validTo = change['valueValidTo'] if 'valueValidTo' in change.keys(\n",
    "                    ) else None\n",
    "                    change_tuples.append((title, pageID, key, template, name, previous_value,\n",
    "                                          current_value, validFrom, validTo, revisionId, user_name, user_id, user_ip, attributes))\n",
    "\n",
    "        data = pd.DataFrame(change_tuples, columns=['pageTitle', 'pageID', 'key', 'template', 'name', 'previousValue',\n",
    "                                                    'currentValue', 'validFrom', 'validTo', 'revisionId', 'user_name', 'user_id', 'user_ip', 'attributes'])\n",
    "        num_change_tuples += len(data)\n",
    "        data = data[(data[\"currentValue\"] != \"\") & (~data[\"currentValue\"].isnull())]\n",
    "        data = data[(data[\"previousValue\"] != \"\") & (~data[\"previousValue\"].isnull())]\n",
    "        data = data[(data[\"validTo\"] != \"\") & (~data[\"validTo\"].isnull())]\n",
    "        data['validFrom'] = pd.to_datetime(data['validFrom'])\n",
    "        data['validTo'] = pd.to_datetime(data['validTo'])\n",
    "\n",
    "        timedeltas_all.extend(data[\"validTo\"]-data['validFrom'])\n",
    "\n",
    "        data[\"isTypo\"] = data.apply(lambda row: typo_check_pandas(\n",
    "            row, words_dict, upper_lev_distance=2, skip_no_alpha=False), axis=1)\n",
    "\n",
    "        timedeltas_typo.extend(data[data[\"isTypo\"]==True][\"validTo\"]-data[data[\"isTypo\"]==True]['validFrom'])\n",
    "        timedeltas_levensh.extend(data[~data[\"isTypo\"].isnull()][\"validTo\"]-data[~data[\"isTypo\"].isnull()]['validFrom'])\n",
    "\n",
    "    number_of_files_start += number_of_files\n",
    "\n",
    "# upper_lev_dist=2\n",
    "# for i in tqdm(range(len(change_tuples))):\n",
    "#     # Check only changes (no creations/deletions)\n",
    "#     if(is_not_empty_or_none(change_tuples[i][5]) and is_not_empty_or_none(change_tuples[i][6])):\n",
    "#         typo_lst.append(typo_check(\n",
    "#             change_tuples[i][5], change_tuples[i][6], words_dict, upper_lev_dist, True))\n",
    "\n",
    "print(\"Read data:\")\n",
    "print(\"Number of edits:\", num_edits)\n",
    "print(\"Number of change tuples:\", num_change_tuples)\n",
    "print(\"\\nProcessed data:\")\n",
    "# count_typos(typo_lst)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 1/1 [00:38<00:00, 38.99s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Read data:\n",
      "Number of edits: 114795\n",
      "Number of change tuples: 550725\n",
      "\n",
      "Processed data:\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "source": [
    "np.array(timedeltas_all,dtype=np.timedelta64).mean()/np.timedelta64(1,\"D\")"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "434.7692041875579"
      ]
     },
     "metadata": {},
     "execution_count": 38
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "data[(data[\"validFrom\"]!=\"\") | (data[\"validFrom\"]!=None)]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "        pageTitle    pageID          key               template        name  \\\n",
       "0       Shlishkes  14876965  500826571-0  infobox prepared food       image   \n",
       "1       Shlishkes  14876965  500826571-0  infobox prepared food        type   \n",
       "2       Shlishkes  14876965  500826571-0  infobox prepared food      course   \n",
       "3       Shlishkes  14876965  500826571-0  infobox prepared food      served   \n",
       "4       Shlishkes  14876965  500826571-0  infobox prepared food    calories   \n",
       "...           ...       ...          ...                    ...         ...   \n",
       "550720      IOSYS  14905962  275123087-0        infobox company        logo   \n",
       "550721      IOSYS  14905962  275123087-0        infobox company  foundation   \n",
       "550722      IOSYS  14905962  275123087-0        infobox company  foundation   \n",
       "550723      IOSYS  14905962  275123087-0        infobox company  foundation   \n",
       "550724       COPG  14798721  721169631-0           infobox_gene    template   \n",
       "\n",
       "                                           previous_value  \\\n",
       "0                                                    None   \n",
       "1                                                    None   \n",
       "2                                                    None   \n",
       "3                                                    None   \n",
       "4                                                    None   \n",
       "...                                                   ...   \n",
       "550720                                [[Image:iosys.jpg]]   \n",
       "550721  [[Sapporo]], [[Japan]] ({{Start date|1998|10|1...   \n",
       "550722  {{Start date|1998|10|10}}<br>[[Sapporo]], [[Ja...   \n",
       "550723  {{Start date|1998|10|10}}<ref name=DTMmag_2008...   \n",
       "550724                                               None   \n",
       "\n",
       "                                            current_value  \\\n",
       "0                                                           \n",
       "1                                            [[Dumpling]]   \n",
       "2                                                           \n",
       "3                                                           \n",
       "4                                                           \n",
       "...                                                   ...   \n",
       "550720                                          iosys.jpg   \n",
       "550721  {{Start date|1998|10|10}}<br>[[Sapporo]], [[Ja...   \n",
       "550722  {{Start date|1998|10|10}}<ref name=DTMmag_2008...   \n",
       "550723  {{Start date|1998|10|10}}<ref name=DTMmag_2008...   \n",
       "550724                                       infobox_gene   \n",
       "\n",
       "                       validFrom                   validTo  revisionId  \\\n",
       "0      2012-07-05 17:42:27+00:00                       NaT   500826571   \n",
       "1      2012-07-05 17:42:27+00:00                       NaT   500826571   \n",
       "2      2012-07-05 17:42:27+00:00                       NaT   500826571   \n",
       "3      2012-07-05 17:42:27+00:00                       NaT   500826571   \n",
       "4      2012-07-05 17:42:27+00:00                       NaT   500826571   \n",
       "...                          ...                       ...         ...   \n",
       "550720 2017-01-23 00:03:32+00:00                       NaT   761433732   \n",
       "550721 2018-02-21 17:34:44+00:00 2019-02-19 11:37:01+00:00   826909494   \n",
       "550722 2019-02-19 11:37:01+00:00 2019-03-14 06:04:03+00:00   884079508   \n",
       "550723 2019-03-14 06:04:03+00:00                       NaT   887691026   \n",
       "550724 2016-05-20 04:35:34+00:00                       NaT   721169631   \n",
       "\n",
       "            user_name     user_id user_ip  \\\n",
       "0          Mindmatrix    160367.0    None   \n",
       "1          Mindmatrix    160367.0    None   \n",
       "2          Mindmatrix    160367.0    None   \n",
       "3          Mindmatrix    160367.0    None   \n",
       "4          Mindmatrix    160367.0    None   \n",
       "...               ...         ...     ...   \n",
       "550720      Karunamon    992275.0    None   \n",
       "550721    VibeScepter  32749056.0    None   \n",
       "550722     Darklanlan  15296459.0    None   \n",
       "550723   Citation bot   7903804.0    None   \n",
       "550724  ProteinBoxBot   3991663.0    None   \n",
       "\n",
       "                                               attributes  \n",
       "0       {'image': '', 'country': '[[Hungary]]', 'alter...  \n",
       "1       {'image': '', 'country': '[[Hungary]]', 'alter...  \n",
       "2       {'image': '', 'country': '[[Hungary]]', 'alter...  \n",
       "3       {'image': '', 'country': '[[Hungary]]', 'alter...  \n",
       "4       {'image': '', 'country': '[[Hungary]]', 'alter...  \n",
       "...                                                   ...  \n",
       "550720  {'parent': '', 'location_country': '[[Japan]]'...  \n",
       "550721  {'parent': '', 'location_country': '[[Japan]]'...  \n",
       "550722  {'parent': '', 'location_country': '[[Japan]]'...  \n",
       "550723  {'parent': '', 'location_country': '[[Japan]]'...  \n",
       "550724                                                 {}  \n",
       "\n",
       "[550725 rows x 14 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pageTitle</th>\n",
       "      <th>pageID</th>\n",
       "      <th>key</th>\n",
       "      <th>template</th>\n",
       "      <th>name</th>\n",
       "      <th>previous_value</th>\n",
       "      <th>current_value</th>\n",
       "      <th>validFrom</th>\n",
       "      <th>validTo</th>\n",
       "      <th>revisionId</th>\n",
       "      <th>user_name</th>\n",
       "      <th>user_id</th>\n",
       "      <th>user_ip</th>\n",
       "      <th>attributes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Shlishkes</td>\n",
       "      <td>14876965</td>\n",
       "      <td>500826571-0</td>\n",
       "      <td>infobox prepared food</td>\n",
       "      <td>image</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "      <td>2012-07-05 17:42:27+00:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>500826571</td>\n",
       "      <td>Mindmatrix</td>\n",
       "      <td>160367.0</td>\n",
       "      <td>None</td>\n",
       "      <td>{'image': '', 'country': '[[Hungary]]', 'alter...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Shlishkes</td>\n",
       "      <td>14876965</td>\n",
       "      <td>500826571-0</td>\n",
       "      <td>infobox prepared food</td>\n",
       "      <td>type</td>\n",
       "      <td>None</td>\n",
       "      <td>[[Dumpling]]</td>\n",
       "      <td>2012-07-05 17:42:27+00:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>500826571</td>\n",
       "      <td>Mindmatrix</td>\n",
       "      <td>160367.0</td>\n",
       "      <td>None</td>\n",
       "      <td>{'image': '', 'country': '[[Hungary]]', 'alter...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Shlishkes</td>\n",
       "      <td>14876965</td>\n",
       "      <td>500826571-0</td>\n",
       "      <td>infobox prepared food</td>\n",
       "      <td>course</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "      <td>2012-07-05 17:42:27+00:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>500826571</td>\n",
       "      <td>Mindmatrix</td>\n",
       "      <td>160367.0</td>\n",
       "      <td>None</td>\n",
       "      <td>{'image': '', 'country': '[[Hungary]]', 'alter...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Shlishkes</td>\n",
       "      <td>14876965</td>\n",
       "      <td>500826571-0</td>\n",
       "      <td>infobox prepared food</td>\n",
       "      <td>served</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "      <td>2012-07-05 17:42:27+00:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>500826571</td>\n",
       "      <td>Mindmatrix</td>\n",
       "      <td>160367.0</td>\n",
       "      <td>None</td>\n",
       "      <td>{'image': '', 'country': '[[Hungary]]', 'alter...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Shlishkes</td>\n",
       "      <td>14876965</td>\n",
       "      <td>500826571-0</td>\n",
       "      <td>infobox prepared food</td>\n",
       "      <td>calories</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "      <td>2012-07-05 17:42:27+00:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>500826571</td>\n",
       "      <td>Mindmatrix</td>\n",
       "      <td>160367.0</td>\n",
       "      <td>None</td>\n",
       "      <td>{'image': '', 'country': '[[Hungary]]', 'alter...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>550720</th>\n",
       "      <td>IOSYS</td>\n",
       "      <td>14905962</td>\n",
       "      <td>275123087-0</td>\n",
       "      <td>infobox company</td>\n",
       "      <td>logo</td>\n",
       "      <td>[[Image:iosys.jpg]]</td>\n",
       "      <td>iosys.jpg</td>\n",
       "      <td>2017-01-23 00:03:32+00:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>761433732</td>\n",
       "      <td>Karunamon</td>\n",
       "      <td>992275.0</td>\n",
       "      <td>None</td>\n",
       "      <td>{'parent': '', 'location_country': '[[Japan]]'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>550721</th>\n",
       "      <td>IOSYS</td>\n",
       "      <td>14905962</td>\n",
       "      <td>275123087-0</td>\n",
       "      <td>infobox company</td>\n",
       "      <td>foundation</td>\n",
       "      <td>[[Sapporo]], [[Japan]] ({{Start date|1998|10|1...</td>\n",
       "      <td>{{Start date|1998|10|10}}&lt;br&gt;[[Sapporo]], [[Ja...</td>\n",
       "      <td>2018-02-21 17:34:44+00:00</td>\n",
       "      <td>2019-02-19 11:37:01+00:00</td>\n",
       "      <td>826909494</td>\n",
       "      <td>VibeScepter</td>\n",
       "      <td>32749056.0</td>\n",
       "      <td>None</td>\n",
       "      <td>{'parent': '', 'location_country': '[[Japan]]'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>550722</th>\n",
       "      <td>IOSYS</td>\n",
       "      <td>14905962</td>\n",
       "      <td>275123087-0</td>\n",
       "      <td>infobox company</td>\n",
       "      <td>foundation</td>\n",
       "      <td>{{Start date|1998|10|10}}&lt;br&gt;[[Sapporo]], [[Ja...</td>\n",
       "      <td>{{Start date|1998|10|10}}&lt;ref name=DTMmag_2008...</td>\n",
       "      <td>2019-02-19 11:37:01+00:00</td>\n",
       "      <td>2019-03-14 06:04:03+00:00</td>\n",
       "      <td>884079508</td>\n",
       "      <td>Darklanlan</td>\n",
       "      <td>15296459.0</td>\n",
       "      <td>None</td>\n",
       "      <td>{'parent': '', 'location_country': '[[Japan]]'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>550723</th>\n",
       "      <td>IOSYS</td>\n",
       "      <td>14905962</td>\n",
       "      <td>275123087-0</td>\n",
       "      <td>infobox company</td>\n",
       "      <td>foundation</td>\n",
       "      <td>{{Start date|1998|10|10}}&lt;ref name=DTMmag_2008...</td>\n",
       "      <td>{{Start date|1998|10|10}}&lt;ref name=DTMmag_2008...</td>\n",
       "      <td>2019-03-14 06:04:03+00:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>887691026</td>\n",
       "      <td>Citation bot</td>\n",
       "      <td>7903804.0</td>\n",
       "      <td>None</td>\n",
       "      <td>{'parent': '', 'location_country': '[[Japan]]'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>550724</th>\n",
       "      <td>COPG</td>\n",
       "      <td>14798721</td>\n",
       "      <td>721169631-0</td>\n",
       "      <td>infobox_gene</td>\n",
       "      <td>template</td>\n",
       "      <td>None</td>\n",
       "      <td>infobox_gene</td>\n",
       "      <td>2016-05-20 04:35:34+00:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>721169631</td>\n",
       "      <td>ProteinBoxBot</td>\n",
       "      <td>3991663.0</td>\n",
       "      <td>None</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>550725 rows × 14 columns</p>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# get idx of all fixed typos\n",
    "typo_idx = []\n",
    "for i in range(len(typo_lst)):\n",
    "    if typo_lst[i] == True:\n",
    "        typo_idx.append(i)\n",
    "\n",
    "matching_lev_idx = []\n",
    "for i in range(len(typo_lst)):\n",
    "    if typo_lst[i] == True or typo_lst[i] == False:\n",
    "        matching_lev_idx.append(i)\n",
    "\n",
    "all_updates_idx = []\n",
    "for i in range(len(typo_lst)):\n",
    "    if typo_lst[i] != \"not tested\":\n",
    "        all_updates_idx.append(i)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Time to Change"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def timedeltas_between_changes(typo_idx, change_tuples):\n",
    "    time_deltas = []\n",
    "    for idx in typo_idx:\n",
    "        if change_tuples[idx][7] is not None and change_tuples[idx][8] is not None:\n",
    "            time_delta = datetime.strptime(change_tuples[idx][8], '%Y-%m-%dT%H:%M:%SZ')-datetime.strptime(\n",
    "                change_tuples[idx][7], '%Y-%m-%dT%H:%M:%SZ')\n",
    "            time_deltas.append(time_delta)\n",
    "    return time_deltas"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "\n",
    "def timedelta_to_seconds(arr): return arr.total_seconds()\n",
    "def timedelta_to_hours(arr): return arr.total_seconds()/60/60\n",
    "def timedelta_to_days(arr): return arr.total_seconds()/60/60/24\n",
    "def timedelta_to_days_int(arr): return arr.days\n",
    "\n",
    "timedelta_to_seconds = np.vectorize(timedelta_to_seconds)\n",
    "timedelta_to_hours = np.vectorize(timedelta_to_hours)\n",
    "timedelta_to_days = np.vectorize(timedelta_to_days)\n",
    "timedelta_to_days_int = np.vectorize(timedelta_to_days_int)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# fixed typos\n",
    "time_deltas = timedeltas_between_changes(typo_idx, change_tuples)\n",
    "time_deltas = np.array(time_deltas)\n",
    "print(\"Average Time to change for a typofix\")\n",
    "print(\"Median time in days\", np.median(timedelta_to_days(time_deltas)))\n",
    "print(\"Median time in hours\", np.median(timedelta_to_hours(time_deltas)))\n",
    "print(\"Median time in seconds\", np.median(timedelta_to_seconds(time_deltas)))\n",
    "print(\"timedelta mean and std in days:\", np.mean(\n",
    "    timedelta_to_days(time_deltas)), np.std(timedelta_to_days(time_deltas)))\n",
    "print(\"timedelta mean:\", str(time_deltas.mean()))\n",
    "print(\"number of samples:\", len(time_deltas))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import seaborn as sns\n",
    "sns.boxplot(timedelta_to_days(time_deltas),showfliers=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import seaborn as sns\n",
    "\n",
    "ax = sns.histplot(timedelta_to_days(time_deltas))\n",
    "ax.set(xlabel='time to change a typo in days', ylabel='count', title='Time to change a typo')\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# matching levenshtein distance\n",
    "time_deltas_lev = timedeltas_between_changes(matching_lev_idx, change_tuples)\n",
    "time_deltas_lev = np.array(time_deltas_lev)\n",
    "print(\"Average Time to change for a typofix\")\n",
    "print(\"Median time in days\", np.median(timedelta_to_days(time_deltas_lev)))\n",
    "print(\"Median time in hours\", np.median(timedelta_to_hours(time_deltas_lev)))\n",
    "print(\"Median time in seconds\", np.median(timedelta_to_seconds(time_deltas_lev)))\n",
    "print(\"timedelta mean and std in days:\", np.mean(\n",
    "    timedelta_to_days(time_deltas_lev)), np.std(timedelta_to_days(time_deltas_lev)))\n",
    "print(\"timedelta mean:\", str(time_deltas_lev.mean()))\n",
    "print(\"number of samples:\",len(time_deltas_lev))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "ax = sns.histplot(timedelta_to_days(time_deltas_lev))\n",
    "ax.set(xlabel='time to change a typo in days', ylabel='count', title='Time to change a typo')\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# all tested data\n",
    "time_deltas_tested = timedeltas_between_changes(all_updates_idx, change_tuples)\n",
    "time_deltas_tested = np.array(time_deltas_tested)\n",
    "print(\"Average Time to change for a typofix\")\n",
    "print(\"Median time in days\", np.median(timedelta_to_days(time_deltas_tested)))\n",
    "print(\"Median time in hours\", np.median(timedelta_to_hours(time_deltas_tested)))\n",
    "print(\"Median time in seconds\", np.median(timedelta_to_seconds(time_deltas_tested)))\n",
    "print(\"timedelta mean and std in days:\", np.mean(\n",
    "    timedelta_to_days(time_deltas_tested)), np.std(timedelta_to_days(time_deltas_tested)))\n",
    "print(\"timedelta mean:\", str(time_deltas_tested.mean()))\n",
    "print(\"number of samples:\",len(time_deltas_tested))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dataframe"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data = pd.DataFrame(change_tuples, columns=['pageTitle', 'pageID', 'key', 'template', 'name', 'previous_value',\n",
    "                    'current_value', 'validFrom', 'validTo', 'revisionId', 'user_name', 'user_id', 'user_ip', 'attributes'])\n",
    "data['validFrom'] = pd.to_datetime(data['validFrom'])\n",
    "data['validTo'] = pd.to_datetime(data['validTo'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data.iloc[typo_idx].head(5)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "timedeltas = data[\"validTo\"]-data[\"validFrom\"]\n",
    "timedeltas.median()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data_typo = data.iloc[typo_idx]\n",
    "timedeltas_typo=data_typo[\"validTo\"]-data_typo[\"validFrom\"]\n",
    "timedeltas_typo.median()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Swear words"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def check_swear(str1, str2, words_dict, lowercase=True):\n",
    "    \"\"\" Check if swear got added or removed.\n",
    "        Input:\n",
    "            str1: prev string\n",
    "            str2: curr string\n",
    "        Output:\n",
    "        prev false , curr true : 1 (swear word added)\n",
    "        prev true , curr false : 2 (swear word removed)\n",
    "    \"\"\"\n",
    "    if lowercase:\n",
    "        str1=str1.lower()\n",
    "        str2=str2.lower()\n",
    "\n",
    "    str1_lst=str1.split()\n",
    "    str2_lst=str2.split()\n",
    "\n",
    "    prev_swear=False\n",
    "    curr_swear=False\n",
    "    for string in str1_lst:\n",
    "        if word_in_dict(string, words_dict):\n",
    "            prev_swear=True\n",
    "            break\n",
    "\n",
    "    for string in str2_lst:\n",
    "        if word_in_dict(string, words_dict):\n",
    "            curr_swear=True\n",
    "            break\n",
    "\n",
    "    if (not prev_swear and curr_swear):\n",
    "        # swear word added\n",
    "        return 1\n",
    "    if (prev_swear and not curr_swear):\n",
    "        # swear word removed\n",
    "        return 2\n",
    "    if (prev_swear and  curr_swear):\n",
    "        # swear word in both\n",
    "        return 3\n",
    "    if (not prev_swear and not curr_swear):\n",
    "        # swear word in none\n",
    "        return 0"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "swear_file = open(\"../../../words_swear.txt\", \"r\")\n",
    "swear_dict = set(swear_file.read().split(\"\\n\"))\n",
    "swear_dict.remove(\"nazi\") # nazi is mostly no swear word in the context\n",
    "\n",
    "def is_not_empty_or_none(input):\n",
    "    return input is not None and input is not \"\"\n",
    "\n",
    "\n",
    "swear_lst = []\n",
    "for i in tqdm(range(len(change_tuples))):\n",
    "    if(is_not_empty_or_none(change_tuples[i][5]) and is_not_empty_or_none(change_tuples[i][6])):\n",
    "        swear_lst.append(check_swear(\n",
    "            change_tuples[i][5], change_tuples[i][6], swear_dict))\n",
    "    else:\n",
    "        swear_lst.append(None)\n",
    "\n",
    "\n",
    "counts_swear = {\"Swearwords added\": 0,\n",
    "                \"Swearwords removed\": 0,\n",
    "                \"Swearwords not touched\": 0,\n",
    "                \"Swearwords not found\": 0,\n",
    "                \"create or delete (skipped)\": 0}\n",
    "for test in swear_lst:\n",
    "    if test is 1:\n",
    "        counts_swear[\"Swearwords added\"] += 1\n",
    "    if test is 2:\n",
    "        counts_swear[\"Swearwords removed\"] += 1\n",
    "    if test is 3:\n",
    "        counts_swear[\"Swearwords not touched\"] += 1\n",
    "    if test is 0:\n",
    "        counts_swear[\"Swearwords not found\"] += 1\n",
    "    if test is None:\n",
    "        # prev or curr is None\n",
    "        counts_swear[\"create or delete (skipped)\"] += 1\n",
    "print(counts_swear)\n",
    "\n",
    "idx_swear = [[], []]\n",
    "for i in range(len(swear_lst)):\n",
    "    if swear_lst[i] == 1:\n",
    "        idx_swear[0].append(i)\n",
    "    if swear_lst[i] == 2:\n",
    "        idx_swear[1].append(i)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(\"Swearwords added:\", counts_swear[\"Swearwords added\"])\n",
    "print(\"Swearwords removed:\", counts_swear[\"Swearwords removed\"])\n",
    "print(\"Swearwords not touched:\", counts_swear[\"Swearwords not touched\"])\n",
    "print(\"Swearwords not found:\", counts_swear[\"Swearwords not found\"])\n",
    "print(\"create or delete (skipped):\", counts_swear[\"create or delete (skipped)\"])\n",
    "edit_count = counts_swear[\"Swearwords added\"]+counts_swear[\"Swearwords removed\"] + \\\n",
    "    counts_swear[\"Swearwords not touched\"]+counts_swear[\"Swearwords not found\"]\n",
    "print(\"Toal tuples (only updates without creations/deletions):\", edit_count)\n",
    "print(\"Toal tuples:\", edit_count+counts_swear[\"create or delete (skipped)\"])\n",
    "print(\"Percentage of swear words in edits (only updates without creations/deletions) added and removed:\",\n",
    "      counts_swear[\"Swearwords added\"]/edit_count, counts_swear[\"Swearwords removed\"]/edit_count)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Swear words added"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "time_deltas_swear = timedeltas_between_changes(idx_swear[0], change_tuples)\n",
    "time_deltas_swear = np.array(time_deltas_swear)\n",
    "print(\"Average Time to change for a typofix\")\n",
    "print(\"Median time in days\", np.median(timedelta_to_days(time_deltas_swear)))\n",
    "print(\"Median time in hours\", np.median(timedelta_to_hours(time_deltas_swear)))\n",
    "print(\"Median time in seconds\", np.median(timedelta_to_seconds(time_deltas_swear)))\n",
    "print(\"timedelta mean and std in days:\", np.mean(\n",
    "    timedelta_to_days(time_deltas_swear)), np.std(timedelta_to_days(time_deltas_swear)))\n",
    "print(\"timedelta mean:\", str(time_deltas_swear.mean()))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def removeOutliers(data, percentile):\n",
    "    lower_quartile = np.percentile(data, percentile)\n",
    "    upper_quartile = np.percentile(data, 100-percentile)\n",
    "    if lower_quartile == upper_quartile:\n",
    "        return data\n",
    "    print(lower_quartile, upper_quartile)\n",
    "    data = data[data >= lower_quartile]\n",
    "    data = data[data < upper_quartile]\n",
    "    return data"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# \n",
    "# ax = sns.histplot(timedelta_to_hours(time_deltas_swear))\n",
    "# ax.set(xlabel='time to remove a swear word in days', ylabel='count', title='Time to remove a swear word')\n",
    "# plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data.iloc[idx_swear[0]].head(10)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Swear words removed"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data.iloc[idx_swear[1]].head(10)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.6.9",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.9 64-bit ('mp': conda)"
  },
  "interpreter": {
   "hash": "9d6890af0e7111529245105513a4571ecfc3e378a026bfe7b711a2eb3eb8eca5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}