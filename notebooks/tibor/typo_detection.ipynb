{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import Levenshtein\n",
    "import matplotlib.pyplot as plt\n",
    "import fastDamerauLevenshtein"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# splits string in words\n",
    "def split_strings(str1, str2):\n",
    "    lst = [str1.split()]\n",
    "    lst.append(str2.split())\n",
    "    return lst\n",
    "\n",
    "# checks if wordcount in both strings is equal\n",
    "def same_wordcounts(lst1, lst2):\n",
    "    return (len(lst1) == len(lst2))\n",
    "\n",
    "# deletes non alphabetical characters from string\n",
    "def skip_no_alpha(string):\n",
    "    only_alpha = \"\"\n",
    "    for char in string:\n",
    "        if char.isalpha():\n",
    "            only_alpha += char\n",
    "    return only_alpha\n",
    "\n",
    "# checks in numbers are increments\n",
    "def is_increment(nr1, nr2):\n",
    "    return (nr1+1 == nr2 or nr1-1 == nr2)\n",
    "\n",
    "def is_first_letter_caseswitch(str1,str2):\n",
    "    return (str1[0].isupper() and str2[0].islower() or str1[0].islower() and str2[0].isupper())\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def get_levenshtein_dists(str1, str2):\n",
    "    words = split_strings(str1, str2)\n",
    "    lst = []\n",
    "    if same_wordcounts(words[0], words[1]):\n",
    "        for i in range(len(words[0])):\n",
    "            lst.append(int(fastDamerauLevenshtein.damerauLevenshtein(\n",
    "                words[0][i], words[1][i], similarity=False)))\n",
    "    return lst, words\n",
    "\n",
    "\n",
    "def get_levenshtein_dists_without_nr(str1, str2):\n",
    "    words = split_strings(str1, str2)\n",
    "    lst = []\n",
    "    if same_wordcounts(words[0], words[1]):\n",
    "        for i in range(len(words[0])):\n",
    "            lst.append(int(fastDamerauLevenshtein.damerauLevenshtein(skip_no_alpha(\n",
    "                words[0][i]), skip_no_alpha(words[1][i]), similarity=False)))\n",
    "    return lst, words\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "testcase1 = [\"Hier sind kkeine Fheler\", \"Hier sind keine Fehler\"]\n",
    "print(get_levenshtein_dists(testcase1[0],testcase1[1]))\n",
    "print(get_levenshtein_dists_without_nr(testcase1[0],testcase1[1]))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "testcase2 = [\"9 tests\", \"10 tests\"]\n",
    "print(get_levenshtein_dists(testcase2[0],testcase2[1]))\n",
    "print(get_levenshtein_dists_without_nr(testcase2[0],testcase2[1]))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "my_file = open(\"../../../words_alpha.txt\", \"r\")\n",
    "words_dict=set(my_file.read().split())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def word_in_dict(str1, words_dict):\n",
    "    return str1 in words_dict\n",
    "\n",
    "\n",
    "def is_typo_fixed(str1, str2, words_dict, lowercase=True):\n",
    "    \"\"\" Check if typo is fixed.\n",
    "        return 1: word was not in dict before (misspelled)\n",
    "        return 2: word with swapped first letter (and other changes depending on edit distance)\n",
    "    \"\"\"\n",
    "    if is_first_letter_caseswitch(str1,str2):\n",
    "        return 2\n",
    "\n",
    "    if lowercase:\n",
    "        str1=str1.lower()\n",
    "        str2=str2.lower()\n",
    "\n",
    "    if (not word_in_dict(str1, words_dict) and word_in_dict(str2, words_dict)):\n",
    "        return 1\n",
    "        \n",
    "    return 999"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def check_for_typo_type(str1, str2, words_dict):\n",
    "    levenshtein_dists, words = get_levenshtein_dists(\n",
    "        str1, str2)\n",
    "    typo_fixed_lst = []\n",
    "    for i in range(len(levenshtein_dists)):\n",
    "        # if(levenshtein_dists[i] > 0)\n",
    "        if(levenshtein_dists[i] > 0 and levenshtein_dists[i] <= 2):\n",
    "            typo_fixed_lst.append(is_typo_fixed(\n",
    "                words[0][i], words[1][i], words_dict))\n",
    "    return typo_fixed_lst"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "testcase1 = [\"Hier sind kkeine Fheler\", \"Hier sind keine Fehler\"]\n",
    "testcase1_en = [\"There are nno erorrs\", \"There are no errors\"]\n",
    "typo_lst = check_for_typo_type(testcase1_en[0], testcase1_en[1], words_dict)\n",
    "print(typo_lst)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def typo_check(str1, str2, words_dict):\n",
    "    \"\"\"Return True if typo\n",
    "       Return False if no typo\n",
    "       Return None if no edit distance is found or edit distance is >2\n",
    "    \"\"\"\n",
    "    typo_lst = check_for_typo_type(str1, str2, words_dict)\n",
    "    if (len(typo_lst) == 0):\n",
    "        return None\n",
    "    for typo_type in typo_lst:\n",
    "        if typo_type > 2:  # 1 is previous not in dict, current is in dict, 2 case switch on first letter\n",
    "            return False\n",
    "    return True"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "input_data = Path(\"../../matched-infoboxes-extracted/\")\n",
    "inp = list(input_data.rglob('*.json'))\n",
    "files = [x for x in inp if x.is_file()]\n",
    "len(files) "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "num_edits = 0\n",
    "change_tuples = []\n",
    "for file in tqdm(files[:20]):\n",
    "    with open(file, 'r', encoding='utf-8') as f:\n",
    "        for jsonObj in f:\n",
    "            single_edit = json.loads(jsonObj)\n",
    "            num_edits += 1\n",
    "            title = single_edit['pageTitle']\n",
    "            pageID = single_edit['pageID']\n",
    "            key = single_edit['key']\n",
    "            template = single_edit['template'] if 'template' in single_edit.keys(\n",
    "            ) else None\n",
    "            changes = single_edit['changes']\n",
    "            timestamp = single_edit['validFrom']\n",
    "            revisionId = single_edit['revisionId']\n",
    "            attributes = single_edit['attributes'] if 'attributes' in single_edit.keys(\n",
    "            ) else None\n",
    "            # print(single_edit['user'])\n",
    "            user_name = single_edit['user']['username'] if 'username' in single_edit['user'].keys(\n",
    "            ) else None\n",
    "            user_id = single_edit['user']['id'] if 'id' in single_edit['user'].keys(\n",
    "            ) else None\n",
    "            user_ip = single_edit['user']['ip'] if 'ip' in single_edit['user'].keys(\n",
    "            ) else None\n",
    "            for change in changes:\n",
    "                name = change['property']['name']\n",
    "                current_value = change['currentValue'] if 'currentValue' in change.keys(\n",
    "                ) else None\n",
    "                previous_value = change['previousValue'] if 'previousValue' in change.keys(\n",
    "                ) else None\n",
    "                change_tuples.append((title, pageID, key, template, name, previous_value,\n",
    "                                     current_value, timestamp, revisionId, user_name, user_id, user_ip, attributes))\n",
    "print(\"Number of edits:\", num_edits)\n",
    "print(\"Number of change tuples:\", len(change_tuples))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "my_file = open(\"../../../words_alpha.txt\", \"r\")\n",
    "words_dict=set(my_file.read().split())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "typo_lst = []\n",
    "for i in tqdm(range(len(change_tuples))):\n",
    "    if(change_tuples[i][5] is not None and change_tuples[i][6] is not None):\n",
    "        typo_lst.append(typo_check(\n",
    "            change_tuples[i][5], change_tuples[i][6], words_dict))\n",
    "    else:\n",
    "        typo_lst.append(None)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "counts = [0, 0, 0]\n",
    "for test in typo_lst:\n",
    "    if test is True:\n",
    "        counts[0] = counts[0]+1\n",
    "    if test is False:\n",
    "        counts[1] = counts[1]+1\n",
    "    if test is None:\n",
    "        counts[2] = counts[2]+1\n",
    "print(counts)\n",
    "print(counts[0]+counts[1]+counts[2])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "idx = []\n",
    "for i in range(len(typo_lst)):\n",
    "    if typo_lst[i] == True:\n",
    "        idx.append(i)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dataframe"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data = pd.DataFrame(change_tuples, columns=['pageTitle', 'pageID', 'key', 'template', 'name', 'previous_value', 'current_value', 'timestamp', 'revisionId', 'user_name', 'user_id', 'user_ip', 'attributes'])\n",
    "data['timestamp'] = pd.to_datetime(data['timestamp'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data.iloc[idx].head(50)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Swear words"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def check_swear(str1, str2, words_dict, lowercase=True):\n",
    "    \"\"\" Check if swear got added or removed.\n",
    "        Input:\n",
    "            str1: prev string\n",
    "            str2: curr string\n",
    "        Output:\n",
    "        prev false , curr true : 1 (swear word added)\n",
    "        prev true , curr false : 2 (swear word removed)\n",
    "    \"\"\"\n",
    "    if lowercase:\n",
    "        str1=str1.lower()\n",
    "        str2=str2.lower()\n",
    "\n",
    "    str1_lst=str1.split()\n",
    "    str2_lst=str2.split()\n",
    "\n",
    "    prev_swear=False\n",
    "    curr_swear=False\n",
    "    for string in str1_lst:\n",
    "        if word_in_dict(string, words_dict):\n",
    "            prev_swear=True\n",
    "            break\n",
    "\n",
    "    for string in str2_lst:\n",
    "        if word_in_dict(string, words_dict):\n",
    "            curr_swear=True\n",
    "            break\n",
    "\n",
    "    if (not prev_swear and curr_swear):\n",
    "        # swear word added\n",
    "        return 1\n",
    "    if (prev_swear and not curr_swear):\n",
    "        # swear word removed\n",
    "        return 2\n",
    "    if (prev_swear and  curr_swear):\n",
    "        # swear word in both\n",
    "        return 3\n",
    "    if (not prev_swear and not curr_swear):\n",
    "        # swear word in none\n",
    "        return 0"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "swear_file = open(\"../../../words_swear.txt\", \"r\")\n",
    "swear_dict = set(swear_file.read().split())\n",
    "\n",
    "swear_lst = []\n",
    "for i in tqdm(range(len(change_tuples))):\n",
    "    if(change_tuples[i][5] is not None and change_tuples[i][6] is not None):\n",
    "        swear_lst.append(check_swear(\n",
    "            change_tuples[i][5], change_tuples[i][6], swear_dict))\n",
    "    else:\n",
    "        swear_lst.append(None)\n",
    "\n",
    "\n",
    "counts_swear = [0, 0, 0, 0, 0]\n",
    "for test in swear_lst:\n",
    "    if test is 1:\n",
    "        counts_swear[0] = counts_swear[0]+1\n",
    "    if test is 2:\n",
    "        counts_swear[1] = counts_swear[1]+1\n",
    "    if test is 3:\n",
    "        counts_swear[2] = counts_swear[2]+1\n",
    "    if test is 0:\n",
    "        counts_swear[3] = counts_swear[3]+1\n",
    "    if test is None:\n",
    "        # prev or curr is None\n",
    "        counts_swear[4] = counts_swear[4]+1\n",
    "print(counts_swear)\n",
    "print(counts_swear[0]+counts_swear[1]+counts_swear[2]+counts_swear[3])\n",
    "\n",
    "idx_swear = [[], []]\n",
    "for i in range(len(swear_lst)):\n",
    "    if swear_lst[i] == 1:\n",
    "        idx_swear[0].append(i)\n",
    "    if swear_lst[i] == 2:\n",
    "        idx_swear[1].append(i)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(\"Swear words added:\", counts_swear[0])\n",
    "print(\"Swear words removed:\", counts_swear[1])\n",
    "print(\"Swear words not touched:\", counts_swear[2])\n",
    "print(\"Swear words not found:\", counts_swear[3])\n",
    "print(\"create or delete (skipped):\", counts_swear[4])\n",
    "edit_count = counts_swear[0]+counts_swear[1]+counts_swear[2]+counts_swear[3]\n",
    "print(\"Toal words (without create/delete):\", edit_count)\n",
    "print(\"Percentage of swear words in edits added and removed:\",\n",
    "      counts_swear[0]/edit_count, counts_swear[1]/edit_count)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Swear words added"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# todo: how handle empty strings?\n",
    "data.iloc[idx_swear[0]].head(50)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Swear words removed"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data.iloc[idx_swear[1]].head(50)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.6.9",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.9 64-bit ('mp': conda)"
  },
  "interpreter": {
   "hash": "9d6890af0e7111529245105513a4571ecfc3e378a026bfe7b711a2eb3eb8eca5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}