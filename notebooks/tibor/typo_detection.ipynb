{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import Levenshtein\n",
    "import matplotlib.pyplot as plt\n",
    "import fastDamerauLevenshtein"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from spellchecker import SpellChecker\n",
    "\n",
    "spell = SpellChecker()\n",
    "# spell.word_frequency.load_text_file(\"../../../words_alpha.txt\")\n",
    "misspelled = spell.unknown([\"123\",\"Steffen\",\"hhouse\"])\n",
    "misspelled"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### How does it work\n",
    "Typos:\n",
    "- previous and current values are splittet into words with non letters/whitespaces removed, if any are None or empty they are skipped\n",
    "- only same wordcounts are tested (it is expected that words stay in the same order)\n",
    "- words are compared to the words with the same index with Damerau-Levenshtein edit distance (swaps are cost 1 not 2)\n",
    "- if any word has edit distance 1 or 2 it is further looked at\n",
    "    1. test if first letter is a case swap (it is expected the user knows what is correct)\n",
    "    2. test if previous word is not in a dictionary but current word is (comparison is done in lowercase)\n",
    "- if any of the tests is true the change is marked as typo-fix\n",
    "\n",
    "Swear words:\n",
    "- previous and current values are splittet into words, if any are None or empty they are skipped\n",
    "- words are compared to a swear word dictionary (https://github.com/RobertJGabriel/Google-profanity-words/blob/master/list.txt)\n",
    "- if any word is matched the value is flagged as swear word\n",
    "- if previous value has no swear word but current value has -> swear word added\n",
    "- if previous value has a swear word but current value has not -> swear word deleted"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# splits string in words\n",
    "def split_strings(str1, str2):\n",
    "    lst = [str1.split()]\n",
    "    lst.append(str2.split())\n",
    "    return lst\n",
    "\n",
    "# checks if wordcount in both strings is equal\n",
    "\n",
    "\n",
    "def same_wordcounts(lst1, lst2):\n",
    "    return (len(lst1) == len(lst2))\n",
    "\n",
    "# deletes non alphabetical characters from string\n",
    "\n",
    "\n",
    "def skip_no_alpha(string):\n",
    "    only_alpha = \"\"\n",
    "    for char in string:\n",
    "        if char.isalpha() or char == \" \":\n",
    "            only_alpha += char\n",
    "    return only_alpha\n",
    "\n",
    "# checks in numbers are increments\n",
    "\n",
    "\n",
    "def is_increment(nr1, nr2):\n",
    "    return (nr1+1 == nr2 or nr1-1 == nr2)\n",
    "\n",
    "# checks if case (upper/loewr) of the first latter is switched\n",
    "\n",
    "\n",
    "def is_first_letter_caseswitch(str1, str2):\n",
    "    return (str1[0].isupper() and str2[0].islower() or str1[0].islower() and str2[0].isupper())\n",
    "\n",
    "\n",
    "def is_not_empty_or_none(input):\n",
    "    return input is not None and input is not \"\"\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def get_levenshtein_dists(lst1, lst2):\n",
    "    if len(lst1) != len(lst2):\n",
    "        print(\"Difference words counts of lists!\")\n",
    "        return\n",
    "    dists = []\n",
    "    for i in range(len(lst1)):\n",
    "            dists.append(int(fastDamerauLevenshtein.damerauLevenshtein(\n",
    "                lst1[i], lst2[i], similarity=False)))\n",
    "    return dists\n",
    "\n",
    "# splits strings in words\n",
    "def get_words_and_dists(str1, str2, only_alpha=False):\n",
    "    if only_alpha:\n",
    "        str1=skip_no_alpha(str1)\n",
    "        str2=skip_no_alpha(str2)\n",
    "    words = split_strings(str1, str2)\n",
    "    if len(words[0]) == len(words[1]):\n",
    "        dists = get_levenshtein_dists(words[0], words[1])\n",
    "    else:\n",
    "        dists = []\n",
    "    return words, dists\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def word_in_dict(str1, words_dict):\n",
    "    return str1 in words_dict\n",
    "\n",
    "\n",
    "def all_words_in_dict(str1, words_dict):\n",
    "    lst = skip_no_alpha(str1).lower().split()\n",
    "    for string in lst:\n",
    "        if(not word_in_dict(string,words_dict)):\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def no_words_in_dict(str1, words_dict):\n",
    "    lst = str1.lower().split()\n",
    "    for string in lst:\n",
    "        if(word_in_dict(string,words_dict)):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def is_typo_fixed(str1, str2, words_dict, lowercase=True):\n",
    "    \"\"\" Check if typo is fixed.\n",
    "        return 0: no other case is found\n",
    "        return 1: word was not in dict before (missspelled)\n",
    "        return 2: word with swapped first letter (and other changes depending on edit distance)\n",
    "    \"\"\"\n",
    "    # detects number errors (dreher,tippfehler), skipps increments. Only works if skip_no_alpha is false\n",
    "    if str1.isdigit() and str2.isdigit() and not is_increment(int(str1), int(str2)):\n",
    "        return 3\n",
    "\n",
    "    if is_first_letter_caseswitch(str1, str2):\n",
    "        return 2\n",
    "\n",
    "    if lowercase:\n",
    "        str1 = str1.lower()\n",
    "        str2 = str2.lower()\n",
    "\n",
    "    # checks if str1 is not in dict but str2 is\n",
    "    if (not word_in_dict(str1, words_dict) and word_in_dict(str2, words_dict)):\n",
    "        return 1\n",
    "\n",
    "    return 0\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def get_typo_type(str1, str2, words_dict, upper_lev_distance=2, skip_no_alpha=False):\n",
    "    words, levenshtein_dists = get_words_and_dists(\n",
    "        str1, str2, skip_no_alpha)\n",
    "    typo_lst = []\n",
    "    for i in range(len(levenshtein_dists)):  # only loops if dists are found (word counts are equal)\n",
    "        # only uses distances >0 <=2\n",
    "        if(levenshtein_dists[i] > 0 and levenshtein_dists[i] <= upper_lev_distance):\n",
    "            typo_lst.append(is_typo_fixed(\n",
    "                words[0][i], words[1][i], words_dict))\n",
    "        # else:  # appends None if dist is <0 or >2\n",
    "        #     typo_lst.append(None)\n",
    "    return typo_lst\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "my_file = open(\"../../../words_alpha.txt\", \"r\")\n",
    "words_dict=set(my_file.read().split(\"\\n\"))\n",
    "\n",
    "testcase1 = [\"Hier sind kkeine Fheler\", \"Hier sind keine Fehler\"]\n",
    "testcase1_en = [\"There are nno erorrs\", \"There are no errors\"]\n",
    "typo_lst = get_typo_type(testcase1_en[0], testcase1_en[1], words_dict)\n",
    "print(typo_lst)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def typo_check(str1, str2, words_dict, upper_lev_distance=2, skip_no_alpha=False):\n",
    "    \"\"\"Return True if typo\n",
    "       Return False if no typo\n",
    "    \"\"\"\n",
    "    typo_lst = get_typo_type(str1, str2, words_dict, upper_lev_distance, skip_no_alpha)\n",
    "    if len(typo_lst) == 0:\n",
    "        return None\n",
    "    for typo_type in typo_lst:\n",
    "        if typo_type > 0:  # 1 is previous not in dict, current is in dict, 2 case switch on first letter\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def typo_check_pandas(row, words_dict, upper_lev_distance=2, skip_no_alpha=False):\n",
    "    \"\"\"Return True if typo\n",
    "       Return False if no typo\n",
    "    \"\"\"\n",
    "    typo_lst = get_typo_type(row[\"currentValue\"], row[\"previousValue\"], words_dict, upper_lev_distance, skip_no_alpha)\n",
    "    if len(typo_lst) == 0:\n",
    "        return None\n",
    "    for typo_type in typo_lst:\n",
    "        if typo_type > 0:  # 1 is previous not in dict, current is in dict, 2 case switch on first letter\n",
    "            return True\n",
    "    return False"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def swear_check(str1, str2, words_dict, lowercase=True):\n",
    "    \"\"\" Check if swear got added or removed.\n",
    "        Input:\n",
    "            str1: prev string\n",
    "            str2: curr string\n",
    "        Output:\n",
    "        prev false , curr true : 1 (swear word added)\n",
    "        prev true , curr false : 2 (swear word removed)\n",
    "    \"\"\"\n",
    "    if lowercase:\n",
    "        str1=str1.lower()\n",
    "        str2=str2.lower()\n",
    "\n",
    "    str1_lst=str1.split()\n",
    "    str2_lst=str2.split()\n",
    "\n",
    "    prev_swear=False\n",
    "    curr_swear=False\n",
    "    for string in str1_lst:\n",
    "        if word_in_dict(string, words_dict):\n",
    "            prev_swear=True\n",
    "            break\n",
    "\n",
    "    for string in str2_lst:\n",
    "        if word_in_dict(string, words_dict):\n",
    "            curr_swear=True\n",
    "            break\n",
    "\n",
    "    if (not prev_swear and curr_swear):\n",
    "        # swear word added\n",
    "        return 1\n",
    "    if (prev_swear and not curr_swear):\n",
    "        # swear word removed\n",
    "        return 2\n",
    "    if (prev_swear and  curr_swear):\n",
    "        # swear word in both\n",
    "        return 3\n",
    "    if (not prev_swear and not curr_swear):\n",
    "        # swear word in none\n",
    "        return 0\n",
    "\n",
    "def swear_check_pandas(row, words_dict, lowercase=True):\n",
    "    return swear_check(row[\"currentValue\"], row[\"previousValue\"], words_dict, lowercase=lowercase)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "\n",
    "def timedelta_to_seconds(arr): return arr.total_seconds()\n",
    "def timedelta_to_hours(arr): return arr.total_seconds()/60/60\n",
    "def timedelta_to_days(arr): return arr.total_seconds()/60/60/24\n",
    "def timedelta_to_days_int(arr): return arr.days\n",
    "\n",
    "timedelta_to_seconds = np.vectorize(timedelta_to_seconds)\n",
    "timedelta_to_hours = np.vectorize(timedelta_to_hours)\n",
    "timedelta_to_days = np.vectorize(timedelta_to_days)\n",
    "timedelta_to_days_int = np.vectorize(timedelta_to_days_int)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "input_data = Path.home()/\"output-infobox\"\n",
    "inp = list(input_data.rglob('*.json'))\n",
    "files = [x for x in inp if x.is_file()]\n",
    "print(\"number of files:\", len(files))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# english words dict\n",
    "my_file = open(\"../../../words_alpha.txt\", \"r\")\n",
    "words_dict=set(my_file.read().split(\"\\n\"))\n",
    "\n",
    "# swear words dict\n",
    "swear_file = open(\"../../../words_swear.txt\", \"r\")\n",
    "swear_dict = set(swear_file.read().split(\"\\n\"))\n",
    "swear_dict.remove(\"nazi\") # nazi is mostly no swear word in the context"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "number_of_files = 1\n",
    "num_edits = 0\n",
    "num_change_tuples = 0\n",
    "timedeltas_all = []\n",
    "timedeltas_wordsNotInDict = []\n",
    "timedeltas_swearWordinCurrentValue = []\n",
    "timedeltas_typo = []\n",
    "timedeltas_levensh = []\n",
    "timedeltas_swear_added = []\n",
    "timedeltas_swear_removed = []\n",
    "for file in tqdm(files[:number_of_files]):\n",
    "    change_tuples = []\n",
    "    with open(file, 'r', encoding='utf-8') as f:\n",
    "        for jsonObj in f:\n",
    "            single_edit = json.loads(jsonObj)\n",
    "            num_edits += 1\n",
    "            title = single_edit['pageTitle']\n",
    "            pageID = single_edit['pageID']\n",
    "            key = single_edit['key']\n",
    "            template = single_edit['template'] if 'template' in single_edit.keys(\n",
    "            ) else None\n",
    "            changes = single_edit['changes']\n",
    "            validFrom = single_edit['validFrom']\n",
    "            revisionId = single_edit['revisionId']\n",
    "            attributes = single_edit['attributes'] if 'attributes' in single_edit.keys(\n",
    "            ) else None\n",
    "            user_name = single_edit['user']['username'] if 'username' in single_edit['user'].keys(\n",
    "            ) else None\n",
    "            user_id = single_edit['user']['id'] if 'id' in single_edit['user'].keys(\n",
    "            ) else None\n",
    "            user_ip = single_edit['user']['ip'] if 'ip' in single_edit['user'].keys(\n",
    "            ) else None\n",
    "            for change in changes:\n",
    "                name = change['property']['name']\n",
    "                current_value = change['currentValue'] if 'currentValue' in change.keys(\n",
    "                ) else None\n",
    "                previous_value = change['previousValue'] if 'previousValue' in change.keys(\n",
    "                ) else None\n",
    "                validTo = change['valueValidTo'] if 'valueValidTo' in change.keys(\n",
    "                ) else None\n",
    "                change_tuples.append((title, pageID, key, template, name, previous_value,\n",
    "                                      current_value, validFrom, validTo, revisionId, user_name, user_id, user_ip, attributes))\n",
    "    # data preprocessing\n",
    "    data_raw = pd.DataFrame(change_tuples, columns=['pageTitle', 'pageID', 'key', 'template', 'name', 'previousValue',\n",
    "                                                    'currentValue', 'validFrom', 'validTo', 'revisionId', 'user_name', 'user_id', 'user_ip', 'attributes'])\n",
    "    num_change_tuples += len(data_raw)\n",
    "    data = data_raw[(data_raw[\"currentValue\"] != \"\") &\n",
    "                    (~data_raw[\"currentValue\"].isnull())]\n",
    "    data = data[(data[\"previousValue\"] != \"\") & (~data[\"previousValue\"].isnull())]\n",
    "    data = data[(data[\"validTo\"] != \"\") & (~data[\"validTo\"].isnull())]\n",
    "    data['validFrom'] = pd.to_datetime(data['validFrom'])\n",
    "    data['validTo'] = pd.to_datetime(data['validTo'])\n",
    "    data_raw['validFrom'] = pd.to_datetime(data_raw['validFrom'])\n",
    "    data_raw['validTo'] = pd.to_datetime(data_raw['validTo'])\n",
    "\n",
    "    data[\"typo\"] = data.apply(lambda row: len(spell.unknown(skip_no_alpha(row[\"currentValue\"]).split(\" \"))) , axis=1)\n",
    "    data[\"typo_data\"] = data.apply(lambda row: spell.unknown(skip_no_alpha(row[\"currentValue\"]).split(\" \")) , axis=1)\n",
    "\n",
    "    # timedeltas_all.extend(data[\"validTo\"]-data['validFrom'])\n",
    "\n",
    "    # data[\"isTypo\"] = data.apply(lambda row: typo_check_pandas(\n",
    "    #     row, words_dict, upper_lev_distance=2, skip_no_alpha=True), axis=1)\n",
    "\n",
    "    # data[\"allWordsInDict\"] = data.apply(\n",
    "    #     lambda row: all_words_in_dict(row[\"currentValue\"], words_dict), axis=1)\n",
    "    # timedeltas_wordsNotInDict.extend(\n",
    "    #     data[data[\"allWordsInDict\"] == False][\"validTo\"]-data[data[\"allWordsInDict\"] == False]['validFrom'])\n",
    "\n",
    "    # data[\"swearWordinCurrentValue\"] = data.apply(\n",
    "    #     lambda row: no_words_in_dict(row[\"currentValue\"], swear_dict), axis=1)\n",
    "    # timedeltas_swearWordinCurrentValue.extend(\n",
    "    #     data[data[\"swearWordinCurrentValue\"] == True][\"validTo\"]-data[data[\"swearWordinCurrentValue\"] == True]['validFrom'])\n",
    "\n",
    "    # lookup for change where typo was inserted (very slow)\n",
    "    # for i, change in data[data[\"isTypo\"] == True].iterrows():\n",
    "    #     change_with_typo = data_raw[\n",
    "    #         (data_raw[\"key\"] == data.loc[i][\"key\"])\n",
    "    #         & (data_raw[\"template\"] == data.loc[i][\"template\"])\n",
    "    #         & (data_raw[\"currentValue\"] == data.loc[i][\"previousValue\"])\n",
    "    #         & (data_raw[\"validTo\"] <= data.loc[i][\"validFrom\"])\n",
    "    #     ].sort_values(\"validTo\", ascending=False).iloc[0]\n",
    "    #     timedeltas_typo=data.loc[i][\"validFrom\"] - change_with_typo[\"validFrom\"]\n",
    "\n",
    "    # timedeltas_typo.extend(data[data[\"isTypo\"] == True]\n",
    "    #                        [\"validTo\"]-data[data[\"isTypo\"] == True]['validFrom'])\n",
    "    # timedeltas_levensh.extend(data[~data[\"isTypo\"].isnull()]\n",
    "    #                           [\"validTo\"]-data[~data[\"isTypo\"].isnull()]['validFrom'])\n",
    "\n",
    "    # data[\"swear\"] = data.apply(lambda row: swear_check_pandas(row, swear_dict), axis=1)\n",
    "    # timedeltas_swear_added.extend(\n",
    "    #     data[data[\"swear\"] == 1][\"validTo\"]-data[data[\"swear\"] == 1]['validFrom'])\n",
    "    # timedeltas_swear_removed.extend(\n",
    "    #     data[data[\"swear\"] == 2][\"validTo\"]-data[data[\"swear\"] == 2]['validFrom'])\n",
    "\n",
    "\n",
    "print(\"Read data:\")\n",
    "print(\"Number of edits:\", num_edits)\n",
    "print(\"Number of change tuples:\", num_change_tuples)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data[data[\"typo\"]>0].tail(10)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# i=141\n",
    "# print(data.loc[i])\n",
    "# change_with_typo=data_raw[\n",
    "#     (data_raw[\"key\"]==data.loc[i][\"key\"])\n",
    "#     & (data_raw[\"template\"]==data.loc[i][\"template\"])\n",
    "#     & (data_raw[\"currentValue\"]==data.loc[i][\"previousValue\"])\n",
    "#     & (data_raw[\"validTo\"]<=data.loc[i][\"validFrom\"])\n",
    "#     ].sort_values(\"validTo\",ascending=False).iloc[0]\n",
    "# print(change_with_typo)\n",
    "# data.loc[i][\"validFrom\"] -change_with_typo[\"validFrom\"]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# c = 0\n",
    "# # 5 tuple per s\n",
    "# for i, change in tqdm(data[data[\"isTypo\"] == True].iterrows()):\n",
    "#     change_with_typos = data_raw[\n",
    "#         (data_raw[\"key\"] == data.loc[i][\"key\"])\n",
    "#         & (data_raw[\"template\"] == data.loc[i][\"template\"])\n",
    "#         & (data_raw[\"currentValue\"] == data.loc[i][\"previousValue\"])\n",
    "#         & (data_raw[\"validTo\"] <= data.loc[i][\"validFrom\"])\n",
    "#     ].sort_values(\"validTo\", ascending=False)\n",
    "#     if(len(change_with_typos)==0):\n",
    "#         c+=1\n",
    "#     timedeltas_typo=data.loc[i][\"validFrom\"] - change_with_typo[\"validFrom\"]\n",
    "\n",
    "# print(c,len(data[data[\"isTypo\"] == True]))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#data[data[\"allWordsInDict\"] == False].head(5)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# print(\"data\")\n",
    "# print(\"number of all changes:\", num_change_tuples)\n",
    "# print(\"number of all updates:\", len(timedeltas_all))\n",
    "# print(\"percent of updates:\", len(timedeltas_all)/num_change_tuples*100)\n",
    "# print(\"\\nword not in dict\")\n",
    "# print(\"number of updates with word not in dict:\",len(timedeltas_wordsNotInDict))\n",
    "# print(\"percent of of updates with word not in dict:\", len(timedeltas_wordsNotInDict)/len(timedeltas_all)*100)\n",
    "\n",
    "# print(\"\\nTIMEDELTAS\")\n",
    "# print(\"ALL UPDATES\")\n",
    "# print(\"median of timedelta of in days:\",\n",
    "#       np.median(timedelta_to_days(timedeltas_all)))\n",
    "# print(\"mean timedelta of in days:\",\n",
    "#       timedelta_to_days(timedeltas_all).mean())\n",
    "# print(\"std of timedelta of in days:\",\n",
    "#       timedelta_to_days(timedeltas_all).std())\n",
    "# print(\"\\nWORDS NOT IN DICT\")\n",
    "# print(\"median of timedelta of in days:\",\n",
    "#       np.median(timedelta_to_days(timedeltas_wordsNotInDict)))\n",
    "# print(\"mean timedelta of in days:\",\n",
    "#       timedelta_to_days(timedeltas_wordsNotInDict).mean())\n",
    "# print(\"std of timedelta of in days:\",\n",
    "#       timedelta_to_days(timedeltas_wordsNotInDict).std())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# print(\"swear word in current value\")\n",
    "# print(\"number of updates with swear word in current value:\",len(timedeltas_swearWordinCurrentValue))\n",
    "# print(\"percent of of updates with swear word in current value:\", len(timedeltas_swearWordinCurrentValue)/len(timedeltas_all)*100)\n",
    "\n",
    "# print(\"\\nTIMEDELTAS\")\n",
    "# print(\"median of timedelta of in days:\",\n",
    "#       np.median(timedelta_to_days(timedeltas_swearWordinCurrentValue)))\n",
    "# print(\"mean timedelta of in days:\",\n",
    "#       timedelta_to_days(timedeltas_swearWordinCurrentValue).mean())\n",
    "# print(\"std of timedelta of in days:\",\n",
    "#       timedelta_to_days(timedeltas_swearWordinCurrentValue).std())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# data[data[\"swearWordinCurrentValue\"] == True].tail(5)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(\"data\")\n",
    "print(\"number of all changes:\", num_change_tuples)\n",
    "print(\"number of all updates:\", len(timedeltas_all))\n",
    "print(\"percent of updates:\", len(timedeltas_all)/num_change_tuples*100)\n",
    "print(\"\\nmatching levenshtein dist of 2\")\n",
    "print(\"number of matching levenshtein dist of 2:\", len(timedeltas_levensh))\n",
    "print(\"percent of updates matching levenshtein dist of 2:\", len(timedeltas_levensh)/len(timedeltas_all)*100)\n",
    "print(\"typos\")\n",
    "print(\"\\nnumber of fixed typos:\", len(timedeltas_typo))\n",
    "print(\"percent of updates with typo:\", len(timedeltas_typo)/len(timedeltas_all)*100)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Time to Change"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(\"ALL UPDATES\")\n",
    "print(\"median of timedelta of in days:\",\n",
    "      np.median(timedelta_to_days(timedeltas_all)))\n",
    "print(\"mean timedelta of in days:\",\n",
    "      timedelta_to_days(timedeltas_all).mean())\n",
    "print(\"std of timedelta of in days:\",\n",
    "      timedelta_to_days(timedeltas_all).std())\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(\"LEVENSHTEIN OF 2\")\n",
    "print(\"median of timedelta in days:\",\n",
    "      np.median(timedelta_to_days(timedeltas_levensh)))\n",
    "print(\"mean timedelta in days:\",\n",
    "      timedelta_to_days(timedeltas_levensh).mean())\n",
    "print(\"std of timedelta of in days:\",\n",
    "      timedelta_to_days(timedeltas_levensh).std())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(\"TYPOS\")\n",
    "print(\"median of timedelta in days:\",\n",
    "      np.median(timedelta_to_days(timedeltas_typo)))\n",
    "print(\"mean timedelta in days:\",\n",
    "      timedelta_to_days(timedeltas_typo).mean())\n",
    "print(\"std of timedelta of in days:\",\n",
    "      timedelta_to_days(timedeltas_typo).std())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data[data[\"isTypo\"]==True]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Swear words"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def check_swear(str1, str2, words_dict, lowercase=True):\n",
    "    \"\"\" Check if swear got added or removed.\n",
    "        Input:\n",
    "            str1: prev string\n",
    "            str2: curr string\n",
    "        Output:\n",
    "        prev false , curr true : 1 (swear word added)\n",
    "        prev true , curr false : 2 (swear word removed)\n",
    "    \"\"\"\n",
    "    if lowercase:\n",
    "        str1=str1.lower()\n",
    "        str2=str2.lower()\n",
    "\n",
    "    str1_lst=str1.split()\n",
    "    str2_lst=str2.split()\n",
    "\n",
    "    prev_swear=False\n",
    "    curr_swear=False\n",
    "    for string in str1_lst:\n",
    "        if word_in_dict(string, words_dict):\n",
    "            prev_swear=True\n",
    "            break\n",
    "\n",
    "    for string in str2_lst:\n",
    "        if word_in_dict(string, words_dict):\n",
    "            curr_swear=True\n",
    "            break\n",
    "\n",
    "    if (not prev_swear and curr_swear):\n",
    "        # swear word added\n",
    "        return 1\n",
    "    if (prev_swear and not curr_swear):\n",
    "        # swear word removed\n",
    "        return 2\n",
    "    if (prev_swear and  curr_swear):\n",
    "        # swear word in both\n",
    "        return 3\n",
    "    if (not prev_swear and not curr_swear):\n",
    "        # swear word in none\n",
    "        return 0"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "swear_file = open(\"../../../words_swear.txt\", \"r\")\n",
    "swear_dict = set(swear_file.read().split(\"\\n\"))\n",
    "swear_dict.remove(\"nazi\") # nazi is mostly no swear word in the context\n",
    "\n",
    "def is_not_empty_or_none(input):\n",
    "    return input is not None and input is not \"\"\n",
    "\n",
    "\n",
    "swear_lst = []\n",
    "for i in tqdm(range(len(change_tuples))):\n",
    "    if(is_not_empty_or_none(change_tuples[i][5]) and is_not_empty_or_none(change_tuples[i][6])):\n",
    "        swear_lst.append(check_swear(\n",
    "            change_tuples[i][5], change_tuples[i][6], swear_dict))\n",
    "    else:\n",
    "        swear_lst.append(None)\n",
    "\n",
    "\n",
    "counts_swear = {\"Swearwords added\": 0,\n",
    "                \"Swearwords removed\": 0,\n",
    "                \"Swearwords not touched\": 0,\n",
    "                \"Swearwords not found\": 0,\n",
    "                \"create or delete (skipped)\": 0}\n",
    "for test in swear_lst:\n",
    "    if test is 1:\n",
    "        counts_swear[\"Swearwords added\"] += 1\n",
    "    if test is 2:\n",
    "        counts_swear[\"Swearwords removed\"] += 1\n",
    "    if test is 3:\n",
    "        counts_swear[\"Swearwords not touched\"] += 1\n",
    "    if test is 0:\n",
    "        counts_swear[\"Swearwords not found\"] += 1\n",
    "    if test is None:\n",
    "        # prev or curr is None\n",
    "        counts_swear[\"create or delete (skipped)\"] += 1\n",
    "print(counts_swear)\n",
    "\n",
    "idx_swear = [[], []]\n",
    "for i in range(len(swear_lst)):\n",
    "    if swear_lst[i] == 1:\n",
    "        idx_swear[0].append(i)\n",
    "    if swear_lst[i] == 2:\n",
    "        idx_swear[1].append(i)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(\"Swearwords added:\", counts_swear[\"Swearwords added\"])\n",
    "print(\"Swearwords removed:\", counts_swear[\"Swearwords removed\"])\n",
    "print(\"Swearwords not touched:\", counts_swear[\"Swearwords not touched\"])\n",
    "print(\"Swearwords not found:\", counts_swear[\"Swearwords not found\"])\n",
    "print(\"create or delete (skipped):\", counts_swear[\"create or delete (skipped)\"])\n",
    "edit_count = counts_swear[\"Swearwords added\"]+counts_swear[\"Swearwords removed\"] + \\\n",
    "    counts_swear[\"Swearwords not touched\"]+counts_swear[\"Swearwords not found\"]\n",
    "print(\"Toal tuples (only updates without creations/deletions):\", edit_count)\n",
    "print(\"Toal tuples:\", edit_count+counts_swear[\"create or delete (skipped)\"])\n",
    "print(\"Percentage of swear words in edits (only updates without creations/deletions) added and removed:\",\n",
    "      counts_swear[\"Swearwords added\"]/edit_count, counts_swear[\"Swearwords removed\"]/edit_count)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Swear words added"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "time_deltas_swear = timedeltas_between_changes(idx_swear[0], change_tuples)\n",
    "time_deltas_swear = np.array(time_deltas_swear)\n",
    "print(\"Average Time to change for a typofix\")\n",
    "print(\"Median time in days\", np.median(timedelta_to_days(time_deltas_swear)))\n",
    "print(\"Median time in hours\", np.median(timedelta_to_hours(time_deltas_swear)))\n",
    "print(\"Median time in seconds\", np.median(timedelta_to_seconds(time_deltas_swear)))\n",
    "print(\"timedelta mean and std in days:\", np.mean(\n",
    "    timedelta_to_days(time_deltas_swear)), np.std(timedelta_to_days(time_deltas_swear)))\n",
    "print(\"timedelta mean:\", str(time_deltas_swear.mean()))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def removeOutliers(data, percentile):\n",
    "    lower_quartile = np.percentile(data, percentile)\n",
    "    upper_quartile = np.percentile(data, 100-percentile)\n",
    "    if lower_quartile == upper_quartile:\n",
    "        return data\n",
    "    print(lower_quartile, upper_quartile)\n",
    "    data = data[data >= lower_quartile]\n",
    "    data = data[data < upper_quartile]\n",
    "    return data"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data.iloc[idx_swear[0]].head(10)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Swear words removed"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data.iloc[idx_swear[1]].head(10)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.6.9",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.9 64-bit ('mp': conda)"
  },
  "interpreter": {
   "hash": "9d6890af0e7111529245105513a4571ecfc3e378a026bfe7b711a2eb3eb8eca5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}