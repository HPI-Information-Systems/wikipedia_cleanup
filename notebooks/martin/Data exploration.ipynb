{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a573769",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import libarchive.public\n",
    "import pickle\n",
    "import random\n",
    "import Levenshtein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c889cda-7f3d-411e-979f-2096a593aee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "PERCENTAGE = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e165ccbc-174d-414a-a643-aed653885a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_file_size(file_path: Path):\n",
    "    with libarchive.public.file_reader(str(file_path)) as archive:\n",
    "        size = 0\n",
    "        for entry in archive:\n",
    "            size += entry.size\n",
    "    return size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b8c700-b418-4148-93d5-22e619e1779a",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_folder = Path('../../../data/matched-infoboxes-raw')\n",
    "input_files = list(input_folder.rglob('*.7z'))\n",
    "len(input_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0078fa-27f2-4507-a0e9-87ca5dfbdbe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE change tuples partially on disk.\n",
    "num_iterations = 100 // PERCENTAGE\n",
    "for i in range(num_iterations):\n",
    "    partial_input_files = partial_input_files = input_files[int(len(input_files) / num_iterations * i): int(len(input_files) / num_iterations * (i + 1))]\n",
    "    print(f'reading {int(len(input_files) / num_iterations * i)} to {int(len(input_files) / num_iterations * (i + 1))}')\n",
    "    change_tuples = []\n",
    "    for archive_path in tqdm(partial_input_files):\n",
    "        with libarchive.public.file_reader(str(archive_path)) as archive:\n",
    "            for entry in archive:\n",
    "                content_bytes = bytearray('', encoding='utf_8')\n",
    "                for block in entry.get_blocks():\n",
    "                    content_bytes += block\n",
    "                content = content_bytes.decode(encoding='utf_8')\n",
    "                jsonObjs = content.split('\\n')\n",
    "                for jsonObj in filter(lambda x: x, jsonObjs):\n",
    "                    obj = json.loads(jsonObj)\n",
    "                    title = obj['pageTitle']\n",
    "                    subject = obj['pageID']\n",
    "                    changes = obj['changes']\n",
    "                    valid_from = obj['validFrom']\n",
    "                    for change in changes:\n",
    "                        current_value = change['currentValue'] if 'currentValue' in change.keys() else None\n",
    "                        previous_value = change['previousValue'] if 'previousValue' in change.keys() else None\n",
    "                        name = change['property']['name']\n",
    "                        valid_to = change['valueValidTo'] if 'valueValidTo' in change.keys() else None\n",
    "                        change_tuples.append((subject, title, name, previous_value, current_value, valid_from, valid_to))\n",
    "    \n",
    "    print('writing file')\n",
    "    with open(f'../../../data/raw_change_tuples/partial_change_tuples_part_{i}.pickle', 'wb') as file:\n",
    "        pickle.dump(change_tuples, file)\n",
    "    print('successfully wrote file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183a7103-e6dd-414b-8d20-ddb6433a3288",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(f'../../../data/raw_change_tuples/partial_change_tuples_part_{0}.pickle', 'rb') as file:\n",
    "        change_tuples = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71cbb174-711f-4140-9a47-68c3dd939778",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(change_tuples, columns=['subject', 'title', 'name', 'previous_value', 'current_value', 'valid_from', 'valid_to'])\n",
    "del change_tuples\n",
    "data['valid_from'] = pd.to_datetime(data['valid_from'])\n",
    "data['valid_to'] = pd.to_datetime(data['valid_to'])\n",
    "data['valid_time'] = data['valid_to'] - data['valid_from']\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec15a782-d1d5-4678-acfb-011f9d138ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8474d1c2-2b36-4199-af71-5a3d421fd983",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_times = data['valid_time'][data['valid_time'].notnull()].to_numpy().astype('int64')\n",
    "plt.hist(valid_times, bins=100)\n",
    "plt.yscale('log')\n",
    "plt.title(\"Number of changes per page\")\n",
    "plt.ylabel(\"Valid time, log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d5b8ae-72e5-44e3-9405-2f97a6a486f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "counted_changes = {}\n",
    "for archive_path in tqdm(input_files):\n",
    "    with libarchive.public.file_reader(str(archive_path)) as archive:\n",
    "        for entry in archive:\n",
    "            content_bytes = bytearray('', encoding='utf_8')\n",
    "            for block in entry.get_blocks():\n",
    "                content_bytes += block\n",
    "            content = content_bytes.decode(encoding='utf_8')\n",
    "            jsonObjs = content.split('\\n')\n",
    "            for jsonObj in filter(lambda x: x, jsonObjs):\n",
    "                obj = json.loads(jsonObj)\n",
    "                subject = obj['pageTitle']\n",
    "                changes = obj['changes']\n",
    "                timestamp = obj['validFrom']\n",
    "                if subject not in counted_changes.keys():\n",
    "                    counted_changes[subject] = {}\n",
    "                curr_counted_changes = counted_changes[subject]\n",
    "                for change in changes:\n",
    "                    name = change['property']['name']\n",
    "                    curr_counted_changes[name] = curr_counted_changes[name] + 1 if name in curr_counted_changes.keys() else 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a31ca3-e29b-488e-bda6-0aaeeeaa717e",
   "metadata": {},
   "outputs": [],
   "source": [
    "change_tuples = []\n",
    "for archive_path in tqdm(partial_input_files):\n",
    "    with libarchive.public.file_reader(str(archive_path)) as archive:\n",
    "        for entry in archive:\n",
    "            content_bytes = bytearray('', encoding='utf_8')\n",
    "            for block in entry.get_blocks():\n",
    "                content_bytes += block\n",
    "            content = content_bytes.decode(encoding='utf_8')\n",
    "            jsonObjs = content.split('\\n')\n",
    "            for jsonObj in filter(lambda x: x, jsonObjs):\n",
    "                obj = json.loads(jsonObj)\n",
    "                subject = obj['pageTitle']\n",
    "                changes = obj['changes']\n",
    "                timestamp = obj['validFrom']\n",
    "                for change in changes:\n",
    "                    current_value = change['currentValue'] if 'currentValue' in change.keys() else None\n",
    "                    previous_value = change['previousValue'] if 'previousValue' in change.keys() else None\n",
    "                    name = change['property']['name']\n",
    "                    change_tuples.append((subject, name, previous_value, current_value, timestamp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8811408a",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_edits = 0\n",
    "change_tuples = []\n",
    "for file in tqdm(files[:50]):\n",
    "    with open(file, 'r', encoding='utf-8') as f:\n",
    "        for jsonObj in f:\n",
    "            single_edit = json.loads(jsonObj)\n",
    "            num_edits += 1\n",
    "            #entries.append(single_edit)\n",
    "            title = single_edit['pageTitle']\n",
    "            changes = single_edit['changes']\n",
    "            timestamp = single_edit['validFrom']\n",
    "            for change in changes:\n",
    "                name = change['property']['name']\n",
    "                current_value = change['currentValue'] if 'currentValue' in change.keys() else None\n",
    "                previous_value = change['previousValue'] if 'previousValue' in change.keys() else None\n",
    "                change_tuples.append((title, name, previous_value, current_value, timestamp))\n",
    "print(num_edits) # 1934309 for 50\n",
    "len(change_tuples) # 9715201 for 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd38918-fd8e-44de-be9e-86c4b2fb0cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse Size of Changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e92f747-3065-4edd-b2b4-d67c4e95f371",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# SAVE change tuples partially on disk.\n",
    "num_iterations = 100 // PERCENTAGE\n",
    "for i in range(num_iterations):\n",
    "    partial_input_files = partial_input_files = input_files[int(len(input_files) / num_iterations * i): int(len(input_files) / num_iterations * (i + 1))]\n",
    "    print(f'reading {int(len(input_files) / num_iterations * i)} to {int(len(input_files) / num_iterations * (i + 1))}')\n",
    "    change_tuples = []\n",
    "    for archive_path in tqdm(partial_input_files):\n",
    "        with libarchive.public.file_reader(str(archive_path)) as archive:\n",
    "            for entry in archive:\n",
    "                content_bytes = bytearray('', encoding='utf_8')\n",
    "                for block in entry.get_blocks():\n",
    "                    content_bytes += block\n",
    "                content = content_bytes.decode(encoding='utf_8')\n",
    "                jsonObjs = content.split('\\n')\n",
    "                for jsonObj in filter(lambda x: x, jsonObjs):\n",
    "                    obj = json.loads(jsonObj)\n",
    "                    title = obj['pageTitle']\n",
    "                    subject = obj['pageID']\n",
    "                    changes = obj['changes']\n",
    "                    valid_from = obj['validFrom']\n",
    "                    for change in changes:\n",
    "                        current_value = change['currentValue'] if 'currentValue' in change.keys() else None\n",
    "                        previous_value = change['previousValue'] if 'previousValue' in change.keys() else None\n",
    "                        name = change['property']['name']\n",
    "                        valid_to = change['valueValidTo'] if 'valueValidTo' in change.keys() else None\n",
    "                        change_tuples.append((subject, title, name, previous_value, current_value, valid_from, valid_to))\n",
    "    \n",
    "    print('writing file')\n",
    "    with open(f'../../../data/raw_change_tuples/partial_change_tuples_part_{i}.pickle', 'wb') as file:\n",
    "        pickle.dump(change_tuples, file)\n",
    "    print('successfully wrote file')ta/raw_change_tuples/partial_change_tuples_part_{0}.pickle', 'rb') as file:\n",
    "        change_tuples = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c6df1c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = pd.DataFrame(change_tuples, columns=['subject', 'title', 'name', 'previous_value', 'current_value', 'valid_from', 'valid_to'])\n",
    "del change_tuples\n",
    "data['valid_from'] = pd.to_datetime(data['valid_from'])\n",
    "data['valid_to'] = pd.to_datetime(data['valid_to'])\n",
    "data['valid_time'] = data['valid_to'] - data['valid_from']\n",
    "data['valid_time'] = data['valid_time'] / np.timedelta64(1, 's')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c1c57d-1c13-4c1e-9ac8-939cbcf36964",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "del data['title']\n",
    "del data['name']\n",
    "del data['valid_from']\n",
    "del data['valid_to']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525721e6-f1e0-46e7-948b-300491830a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data['change_size'] = data[['previous_value', 'current_value']].agg(lambda x:print(x.__class__), axis=1)\n",
    "data['change_size'] = data['previous_value'].combine(data['current_value'], lambda x, y: Levenshtein.distance(x, y) if x and y else None, fill_value=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605575fa-aead-4201-929e-5507a62a250c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data.sort_values(by=['valid_time'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64753704-2cd0-4f06-847b-45c6942fd435",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def seconds_to_day(sec):\n",
    "    return sec / (60 * 60 * 24)\n",
    "\n",
    "def day_to_seconds(sec):\n",
    "    return sec * (60 * 60 * 24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ecee6ef-5686-4d05-bfb5-040e38b721b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_time = seconds_to_day(data['valid_time'].dropna().to_numpy())\n",
    "valid_time.min(), valid_time.max(), valid_time.mean(), valid_time.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8dd7f2-8bdf-494e-a803-9c85cc8848a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "recent_changes = data[data['valid_time'] <= day_to_seconds(1)]\n",
    "plt.hist(recent_changes['change_size'].to_numpy(), bins=list(range(20)))\n",
    "plt.title(\"Size of changes (Levenshtein) for changes that held for less than a day\")\n",
    "plt.ylabel(\"#Occurances, log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94c119b-10dd-4a8e-b111-07bbcb9f8474",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data[data['change_size'] <= 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2347aa40-ca8e-472b-8d34-6a938e06b818",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "recent_changes = data[data['valid_time'] >= day_to_seconds(356)]\n",
    "plt.hist(recent_changes['change_size'].to_numpy(), bins=list(range(100)))\n",
    "plt.title(\"Size of changes (Levenshtein) for changes that held at least a year\")\n",
    "plt.ylabel(\"#Occurances, log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553ccf12-e63a-46e2-a405-28252a4339d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# File Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec68f0ce-77b3-4df9-bdbc-c3af7e6a3f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Does ['property']['type'] have another value as 'attribute'\n",
    "types = set()\n",
    "for archive_path in tqdm(input_files):\n",
    "    with libarchive.public.file_reader(str(archive_path)) as archive:\n",
    "        for entry in archive:\n",
    "            content_bytes = bytearray('', encoding='utf_8')\n",
    "            for block in entry.get_blocks():\n",
    "                content_bytes += block\n",
    "            content = content_bytes.decode(encoding='utf_8')\n",
    "            jsonObjs = content.split('\\n')\n",
    "            for jsonObj in filter(lambda x: x, jsonObjs):\n",
    "                obj = json.loads(jsonObj)\n",
    "                changes = obj['changes']\n",
    "                for change in changes:\n",
    "                    prop_type = change['property']['type']\n",
    "                    types.add(prop_type)\n",
    "types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6250db-a7a6-465b-b49c-6f9d61c1ef67",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# How many numeric values\n",
    "numeric = 0\n",
    "string = 0\n",
    "numeric_to_string = 0\n",
    "string_to_numeric = 0\n",
    "for archive_path in tqdm(input_files):\n",
    "    with libarchive.public.file_reader(str(archive_path)) as archive:\n",
    "        for entry in archive:\n",
    "            content_bytes = bytearray('', encoding='utf_8')\n",
    "            for block in entry.get_blocks():\n",
    "                content_bytes += block\n",
    "            content = content_bytes.decode(encoding='utf_8')\n",
    "            jsonObjs = content.split('\\n')\n",
    "            for jsonObj in filter(lambda x: x, jsonObjs):\n",
    "                obj = json.loads(jsonObj)\n",
    "                changes = obj['changes']\n",
    "                for change in changes:\n",
    "                    curr_val_number = None\n",
    "                    prev_val_number = None\n",
    "                    if 'previousValue' in change:\n",
    "                        try:\n",
    "                            float(change['previousValue'])\n",
    "                            prev_val_number = True\n",
    "                        except ValueError:\n",
    "                            prev_val_number = False\n",
    "                    if 'currentValue' in change:iterate\n",
    "                        try:\n",
    "                            float(change['currentValue'])\n",
    "                            curr_val_number = True\n",
    "                        except ValueError:\n",
    "                            curr_val_number = False\n",
    "                    if curr_val_number is not None:\n",
    "                        numeric += int(curr_val_number)\n",
    "                        string += int(not curr_val_number)\n",
    "                        if prev_val_number is not None:\n",
    "                            if curr_val_number and not prev_val_number:\n",
    "                                string_to_numeric += 1\n",
    "                            if not curr_val_number and prev_val_number:\n",
    "                                numeric_to_string += 1\n",
    "                                    \n",
    "print(f'numeric: {numeric} \\t\\t % {numeric / (numeric + string)}')\n",
    "print(f'string: {string} \\t\\t % {string / (numeric + string)}')\n",
    "print('\\n\\nType Changes\\n\\n')\n",
    "print(f'numeric to string: {numeric_to_string} \\t\\t % {numeric_to_string / (numeric + string)}')\n",
    "print(f'string to numeric: {string_to_numeric} \\t\\t % {string_to_numeric / (numeric + string)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c52da6f-8159-428d-8faa-963e98403b4c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Analyse bot reverts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dee5f0b-e082-497f-bd6f-dd96b1e1aa06",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "partial_input_files = partial_input_files = input_files[: int(len(input_files) * (PERCENTAGE / 100))]\n",
    "change_tuples = []\n",
    "for archive_path in tqdm(partial_input_files):\n",
    "    with libarchive.public.file_reader(str(archive_path)) as archive:\n",
    "        for entry in archive:\n",
    "            content_bytes = bytearray('', encoding='utf_8')\n",
    "            for block in entry.get_blocks():\n",
    "                content_bytes += block\n",
    "            content = content_bytes.decode(encoding='utf_8')\n",
    "            jsonObjs = content.split('\\n')\n",
    "            for jsonObj in filter(lambda x: x, jsonObjs):\n",
    "                obj = json.loads(jsonObj)\n",
    "                key = obj['key']\n",
    "                revisionID = obj['revisionId']\n",
    "                valid_from = obj['validFrom']\n",
    "                changes = obj['changes']\n",
    "                for change in changes:\n",
    "                    current_value = change['currentValue'] if 'currentValue' in change.keys() else None\n",
    "                    previous_value = change['previousValue'] if 'previousValue' in change.keys() else None\n",
    "                    name = change['property']['name']\n",
    "                    valid_to = change['valueValidTo'] if 'valueValidTo' in change.keys() else None\n",
    "                    change_tuples.append((key, revisionID, name, previous_value, current_value, valid_from, valid_to))\n",
    "\n",
    "print('writing file')\n",
    "with open(f'../../../data/raw_change_tuples/partial_change_tuples_part_key_{0}.pickle', 'wb') as file:\n",
    "    pickle.dump(change_tuples, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4793719-b99e-4d28-974b-5e458954fe27",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(f'../../../data/raw_change_tuples/partial_change_tuples_part_key_{0}.pickle', 'rb') as file:\n",
    "        change_tuples = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6d3133-ae7d-43e9-b512-11b73f62de5a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = pd.DataFrame(change_tuples, columns=['key', 'revisionId', 'name', 'previous_value', 'current_value', 'valid_from', 'valid_to'])\n",
    "del change_tuples\n",
    "data['valid_from'] = pd.to_datetime(data['valid_from'])\n",
    "data['valid_to'] = pd.to_datetime(data['valid_to'])\n",
    "data['valid_time'] = data['valid_to'] - data['valid_from']\n",
    "data['valid_time'] = data['valid_time'] / np.timedelta64(1, 's')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe1cdb5-81e2-43a3-94d1-e9b27ea5e2ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "groups = data.groupby(by=['key', 'revisionId', 'name'])\n",
    "groups.filter(lambda x: x.shape[0] > 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0f5aeb-5166-4b20-bfab-a0a05a2bc567",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8eed5c-b4fe-4ecc-b491-068634bc99e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, values in list(groups.groups.iteritems())[:20]:\n",
    "    print(df.ix[values], \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b8ba49-93fb-4e23-bf6c-9d499bd28a68",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "groups.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e272f2-2182-4a18-be66-7b955f30a09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse Creations and Deletions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a209e50f-8db1-471c-a448-2bd3c9888fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(input_folder.joinpath('change_sizes.pickle'), 'rb') as file:\n",
    "    change_sizes = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30905706-7bb1-41a2-bed8-b5e84ad8f779",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "change_sizes = np.array(change_sizes)\n",
    "plt.hist(np.array(change_sizes), bins=list(range(100)))\n",
    "#plt.yscale('log')\n",
    "plt.title(\"Size of changes (Levenstein Distance)\")\n",
    "plt.ylabel(\"#Occurances\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05105f9-40ef-47dc-9c73-ac10c91fbeed",
   "metadata": {},
   "outputs": [],
   "source": [
    "change_sizes.min(), change_sizes.max(), change_sizes.mean(), change_sizes.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2776df3d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Analyse changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c34c4b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "changes_per_page = data.groupby(['title'])['timestamp'].count()\n",
    "plt.hist(changes_per_page.to_numpy(), bins=100)\n",
    "plt.yscale('log')\n",
    "plt.title(\"Number of changes per page\")\n",
    "plt.ylabel(\"#Occurances, log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a2e906",
   "metadata": {},
   "outputs": [],
   "source": [
    "changes_per_attribute = data.groupby(['title', 'name'])['timestamp'].count()\n",
    "plt.hist(changes_per_attribute.to_numpy(), bins=100)\n",
    "plt.yscale('log')\n",
    "plt.title(\"Number of changes per attribute\")\n",
    "plt.ylabel(\"#Occurances, log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770da641",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_with_agg = data.join(changes_per_page, on='title', rsuffix='_r').rename(columns={'timestamp_r': 'changes_per_page'})\n",
    "data_with_agg = data_with_agg.join(changes_per_attribute, on=['title', 'name'], rsuffix='_r').rename(columns={'timestamp_r': 'changes_per_attribute'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67aa5e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_with_agg.sort_values('changes_per_attribute')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb68c87",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
