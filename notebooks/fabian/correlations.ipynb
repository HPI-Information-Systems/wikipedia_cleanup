{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4da73e-e3eb-4a2c-a06b-28f566f49493",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext lab_black\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import ticker as tck\n",
    "import seaborn as sns\n",
    "import ipywidgets\n",
    "import pydantic\n",
    "import typing\n",
    "import datetime\n",
    "import itertools\n",
    "import collections\n",
    "import json\n",
    "import gzip\n",
    "import tqdm\n",
    "import multiprocessing\n",
    "from efficient_apriori import apriori\n",
    "import math\n",
    "import locale\n",
    "from scipy.stats import linregress\n",
    "from matplotlib.offsetbox import AnchoredText\n",
    "from datetime import date\n",
    "from IPython.display import HTML\n",
    "\n",
    "locale.setlocale(locale.LC_ALL, \"de_DE\")\n",
    "locale._override_localeconv[\"thousands_sep\"] = \".\"\n",
    "locale._override_localeconv[\"grouping\"] = [3, 3, 0]\n",
    "plt.rcParams[\"axes.formatter.use_locale\"] = True\n",
    "sns.set_theme(style=\"ticks\")\n",
    "cm = 1 / 2.54\n",
    "a4 = 29.7, 42\n",
    "\n",
    "\n",
    "class InfoboxProperty(pydantic.BaseModel):\n",
    "    propertyType: typing.Optional[str]\n",
    "    name: str\n",
    "\n",
    "\n",
    "class InfoboxChange(pydantic.BaseModel):\n",
    "    property: InfoboxProperty\n",
    "    valueValidTo: typing.Optional[datetime.datetime] = None\n",
    "    currentValue: typing.Optional[str] = None\n",
    "    previousValue: typing.Optional[str] = None\n",
    "\n",
    "\n",
    "class User(pydantic.BaseModel):\n",
    "    username: typing.Optional[str]\n",
    "    id: typing.Optional[int]\n",
    "\n",
    "\n",
    "class InfoboxRevision(pydantic.BaseModel):\n",
    "    revisionId: int\n",
    "    pageTitle: str\n",
    "    changes: typing.Sequence[InfoboxChange]\n",
    "    validFrom: datetime.datetime\n",
    "    attributes: typing.Optional[typing.Dict[str, str]]\n",
    "    pageID: int\n",
    "    revisionType: typing.Optional[str]\n",
    "    key: str\n",
    "    template: typing.Optional[str] = None\n",
    "    position: typing.Optional[int] = None\n",
    "    user: typing.Optional[User] = None\n",
    "    validTo: typing.Optional[datetime.datetime] = None\n",
    "\n",
    "\n",
    "class ChangeBuckets(pydantic.BaseModel):\n",
    "    filename: str\n",
    "    changes: typing.Dict[str, typing.Sequence[typing.Hashable]]\n",
    "\n",
    "\n",
    "def sliding(seq, window_size):\n",
    "    for i in range(len(seq) - window_size + 1):\n",
    "        yield seq[i : i + window_size]\n",
    "\n",
    "\n",
    "def overlapping_groups(groups, window_size):\n",
    "    return {\n",
    "        keys[0]: set().union(*(groups[key] for key in keys))\n",
    "        for keys in sliding(tuple(groups.keys()), window_size)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dafdd18-b18e-4726-a5f2-83eaf05cf7b3",
   "metadata": {},
   "source": [
    "# Creating Buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857cb9f0-c60c-4130-bdc2-98ad5723f785",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_pageid(file):\n",
    "    groups = collections.defaultdict(set)\n",
    "    with open(file, \"rb\") as f:\n",
    "        for ibc in pickle.load(f):\n",
    "            groups[ibc.value_valid_from.date().isoformat()].add(ibc.page_id)\n",
    "    return ChangeBuckets(\n",
    "        filename=file.name,\n",
    "        changes={k: tuple(sorted(groups[k])) for k in sorted(groups.keys())},\n",
    "    )\n",
    "\n",
    "\n",
    "def process_property(file):\n",
    "    groups = collections.defaultdict(set)\n",
    "    with open(file) as f:\n",
    "        revisions = (InfoboxRevision.parse_raw(line) for line in f)\n",
    "        for revision in revisions:\n",
    "            groups[revision.validFrom.date().isoformat()].update(\n",
    "                change.property.name for change in revision.changes\n",
    "            )\n",
    "    return ChangeBuckets(\n",
    "        filename=file.name,\n",
    "        changes={k: tuple(sorted(groups[k])) for k in sorted(groups.keys())},\n",
    "    )\n",
    "\n",
    "\n",
    "def process_template_property(file):\n",
    "    groups = collections.defaultdict(set)\n",
    "    with open(file) as f:\n",
    "        revisions = (InfoboxRevision.parse_raw(line) for line in f)\n",
    "        for revision in revisions:\n",
    "            groups[revision.validFrom.date().isoformat()].update(\n",
    "                (str(revision.template), change.property.name)\n",
    "                for change in revision.changes\n",
    "            )\n",
    "    return ChangeBuckets(\n",
    "        filename=file.name,\n",
    "        changes={k: tuple(sorted(groups[k])) for k in sorted(groups.keys())},\n",
    "    )\n",
    "\n",
    "\n",
    "def process_page_property(file):\n",
    "    groups = collections.defaultdict(set)\n",
    "    with open(file) as f:\n",
    "        revisions = (InfoboxRevision.parse_raw(line) for line in f)\n",
    "        for revision in revisions:\n",
    "            groups[revision.validFrom.date().isoformat()].update(\n",
    "                (revision.pageID, change.property.name) for change in revision.changes\n",
    "            )\n",
    "    return ChangeBuckets(\n",
    "        filename=file.name,\n",
    "        changes={k: tuple(sorted(groups[k])) for k in sorted(groups.keys())},\n",
    "    )\n",
    "\n",
    "\n",
    "fname = \"./changesets-pageid-dfe.json.gz\"\n",
    "\n",
    "if not Path(fname).exists():\n",
    "    groups = collections.defaultdict(set)\n",
    "    files = [\n",
    "        x\n",
    "        for x in sorted(\n",
    "            Path(\"../../data/custom-format-default-filtered/\").rglob(\"*.pickle\")\n",
    "        )\n",
    "        if x.is_file()\n",
    "    ]\n",
    "    with multiprocessing.Pool(2) as p:\n",
    "        imap = p.imap(process_pageid, files)\n",
    "        for cb in tqdm.notebook.tqdm(imap, total=len(files)):\n",
    "            for k, v in cb.changes.items():\n",
    "                groups[k].update(v)\n",
    "    del files\n",
    "    groups = {k: tuple(sorted(groups[k])) for k in sorted(groups.keys())}\n",
    "    with open(fname, \"wb\") as f:\n",
    "        f.write(\n",
    "            gzip.compress(\n",
    "                ChangeBuckets(filename=\"all\", changes=groups)\n",
    "                .json(indent=None, separators=(\",\", \":\"))\n",
    "                .encode(\"utf-8\")\n",
    "            )\n",
    "        )\n",
    "else:\n",
    "    with open(fname, \"rb\") as f:\n",
    "        groups = ChangeBuckets.parse_raw(\n",
    "            gzip.decompress(f.read()).decode(\"utf-8\")\n",
    "        ).changes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83854a59-d3c0-4866-af72-f646282df8c1",
   "metadata": {},
   "source": [
    "## Min/Max Support Filtering Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa38ecbf-f2fc-4760-af1c-92e27bfc6a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ipywidgets.interact(\n",
    "    sma=ipywidgets.IntSlider(value=30, min=5, max=365, step=5, continuous_update=False)\n",
    ")\n",
    "def plot_freq(sma):\n",
    "    fig = plt.figure(figsize=(20 * cm, 15 * cm), dpi=100)\n",
    "    ax = plt.subplot(111)\n",
    "    data = pd.DataFrame(\n",
    "        ((key, len(value)) for key, value in groups.items()), columns=[\"Date\", \"Daily\"]\n",
    "    )\n",
    "    data[\"Date\"] = pd.to_datetime(data[\"Date\"])\n",
    "    data[\"Daily\"] /= len(set().union(*groups.values()))\n",
    "    data = data.set_index(\"Date\").resample(\"D\").sum().reset_index()\n",
    "    data[f\"SMA{sma}\"] = data[\"Daily\"].rolling(sma).mean()\n",
    "    data = data.melt(id_vars=[\"Date\"], var_name=\"Data\", value_name=\"Percentage\")\n",
    "\n",
    "    sns.lineplot(x=\"Date\", y=\"Percentage\", hue=\"Data\", data=data)\n",
    "    ax.yaxis.set_major_formatter(tck.PercentFormatter(xmax=1))\n",
    "    sns.despine(ax=ax)\n",
    "    sns.move_legend(\n",
    "        ax,\n",
    "        \"lower center\",\n",
    "        bbox_to_anchor=(0.5, 1),\n",
    "        ncol=2,\n",
    "        title=None,\n",
    "        frameon=False,\n",
    "    )\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4a55b6-6cff-4108-a84d-afaff95fdd9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_freq(g):\n",
    "    freqs = collections.defaultdict(int)\n",
    "    for bucket in g.values():\n",
    "        for id in bucket:\n",
    "            freqs[id] += 1\n",
    "\n",
    "    freqs = pd.DataFrame(freqs.items(), columns=[\"ID\", \"Count\"])\n",
    "    freqs[\"Frequency\"] = freqs[\"Count\"] / len(groups)\n",
    "    return freqs.sort_values([\"Count\", \"ID\"], ascending=[False, True])\n",
    "\n",
    "\n",
    "freqs = get_freq(groups)\n",
    "freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a663f035-0b50-455a-a4a1-88cfa5da8bbc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@ipywidgets.interact(\n",
    "    bucket_size=ipywidgets.IntSlider(\n",
    "        value=1, min=1, max=21, step=1, continuous_update=False\n",
    "    ),\n",
    "    bins=ipywidgets.IntSlider(\n",
    "        value=50, min=5, max=100, step=5, continuous_update=False\n",
    "    ),\n",
    "    lower=ipywidgets.FloatSlider(\n",
    "        value=0.05,\n",
    "        min=0.0,\n",
    "        max=1.0,\n",
    "        step=0.001,\n",
    "        readout_format=\".1%\",\n",
    "        continuous_update=False,\n",
    "    ),\n",
    "    upper=ipywidgets.FloatSlider(\n",
    "        value=0.1,\n",
    "        min=0.0,\n",
    "        max=1.0,\n",
    "        step=0.001,\n",
    "        readout_format=\".1%\",\n",
    "        continuous_update=False,\n",
    "    ),\n",
    ")\n",
    "def plot_frequency_hist(bucket_size, bins, lower, upper):\n",
    "    fig = plt.figure(figsize=(20 * cm, 15 * cm), dpi=100)\n",
    "    ax = plt.subplot(111)\n",
    "    data = get_freq(overlapping_groups(groups, bucket_size))\n",
    "    data[\"Filtered\"] = (data[\"Frequency\"] < lower) | (data[\"Frequency\"] > upper)\n",
    "    sns.histplot(\n",
    "        data=data,\n",
    "        x=\"Frequency\",\n",
    "        hue=\"Filtered\",\n",
    "        stat=\"percent\",\n",
    "        bins=bins,\n",
    "        multiple=\"stack\",\n",
    "        log_scale=(False, True),\n",
    "        ax=ax,\n",
    "    )\n",
    "    sns.move_legend(\n",
    "        ax,\n",
    "        \"center left\",\n",
    "        bbox_to_anchor=(1, 0.5),\n",
    "        ncol=1,\n",
    "        frameon=False,\n",
    "    )\n",
    "    ax.annotate(\n",
    "        f\"Filtered: {data['Filtered'].sum() / len(data):.2%} IDs\",\n",
    "        xy=(1, 1),\n",
    "        xycoords=\"axes fraction\",\n",
    "        xytext=(0, 0),\n",
    "        textcoords=\"offset points\",\n",
    "        ha=\"right\",\n",
    "        va=\"top\",\n",
    "    )\n",
    "    ax.xaxis.set_major_formatter(tck.PercentFormatter(xmax=1))\n",
    "    ax.yaxis.set_major_formatter(tck.PercentFormatter(xmax=100, decimals=4))\n",
    "    sns.despine(ax=ax)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502cf200-c3ae-40af-a45c-3f69f2220bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tss = (\n",
    "    pd.DataFrame(\n",
    "        (\n",
    "            (ts, item)\n",
    "            for ts in sorted(\n",
    "                ts\n",
    "                for ts in groups.keys()\n",
    "                if datetime.date(2018, 9, 1)\n",
    "                < datetime.datetime.strptime(ts, \"%Y-%m-%d\").date()\n",
    "                < datetime.date(2019, 9, 1)\n",
    "            )\n",
    "            for item in groups[ts]\n",
    "        ),\n",
    "        columns=[\"Date\", \"Page ID\"],\n",
    "    )\n",
    "    .set_index(\"Date\")\n",
    "    .sort_index()\n",
    ")\n",
    "tss = tss.value_counts().reset_index().rename(columns={0: \"Changes\"})\n",
    "tss[\"Page ID\"] = tss[\"Page ID\"].apply(\n",
    "    lambda pid: f'<a href=\"https://en.wikipedia.org/?curid={pid}\">{pid}</a>'\n",
    ")\n",
    "\n",
    "HTML(\n",
    "    tss[tss[\"Changes\"].between(80, 200)]\n",
    "    .sample(15)\n",
    "    .sort_values([\"Changes\", \"Page ID\"], ascending=[False, True])\n",
    "    .to_html(escape=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f517a8-15ed-4290-98c9-d9fec522c864",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Apriori Association Rule Mining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ebca635-7318-45cf-b15e-f3c1b84d84b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "n_days = 5\n",
    "min_support = 0.18\n",
    "min_confidence = 0.75\n",
    "max_length = 2\n",
    "\n",
    "data = tuple(overlapping_groups(groups, n_days).values())\n",
    "\n",
    "val_size = math.ceil(len(data) * 0.2)\n",
    "test_size = math.ceil(len(data) * 0.2)\n",
    "train_data = data[: len(data) - (val_size + test_size)]\n",
    "val_data = data[len(data) - (val_size + test_size) : len(data) - test_size]\n",
    "test_data = data[len(data) - test_size :]\n",
    "del data, val_size, test_size\n",
    "\n",
    "itemsets, rules = apriori(\n",
    "    train_data,\n",
    "    min_support=min_support,\n",
    "    min_confidence=min_confidence,\n",
    "    max_length=max_length,\n",
    "    verbosity=1,\n",
    ")\n",
    "del n_days\n",
    "\n",
    "df = (\n",
    "    pd.DataFrame(\n",
    "        [\n",
    "            (\n",
    "                frozenset(rule.rhs),\n",
    "                frozenset(rule.lhs),\n",
    "                rule.confidence,\n",
    "                rule.support,\n",
    "                rule.lift,\n",
    "                rule.conviction,\n",
    "            )\n",
    "            for rule in rules\n",
    "        ],\n",
    "        columns=[\"RHS\", \"LHS\", \"Confidence\", \"Support\", \"Lift\", \"Conviction\"],\n",
    "    )\n",
    "    .set_index([\"RHS\", \"LHS\"])\n",
    "    .sort_index()\n",
    ")\n",
    "display(df.describe().T.style.format(\"{:.2f}\"))\n",
    "display(df.sort_values(\"Lift\", ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d23206d-219d-49fa-bd7b-7806979d668e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "occurences = collections.defaultdict(\n",
    "    lambda: {\n",
    "        \"train_occurences\": 0,\n",
    "        \"train_total\": 0,\n",
    "        \"val_occurences\": 0,\n",
    "        \"val_total\": 0,\n",
    "        \"test_occurences\": 0,\n",
    "        \"test_total\": 0,\n",
    "    }\n",
    ")  # : Dict[ID, ((int, int), (int, int), (int, int))]\n",
    "seen = set()\n",
    "for i, group in tqdm.tqdm(\n",
    "    enumerate(itertools.chain(train_data, val_data, test_data)),\n",
    "    total=len(train_data) + len(val_data) + len(test_data),\n",
    "):\n",
    "    if i < len(train_data):\n",
    "        s, j = \"train\", len(train_data)\n",
    "    elif i < len(train_data) + len(val_data):\n",
    "        s, j = \"val\", len(train_data) + len(val_data)\n",
    "        if i == len(train_data):\n",
    "            seen = set()\n",
    "    elif i < len(train_data) + len(val_data) + len(test_data):\n",
    "        s, j = \"test\", len(train_data) + len(val_data) + len(test_data)\n",
    "        if i == len(train_data) + len(val_data):\n",
    "            seen = set()\n",
    "    for id in group - seen:\n",
    "        occurences[id][f\"{s}_total\"] = j - i\n",
    "    seen |= group\n",
    "    for id in group:\n",
    "        occurences[id][f\"{s}_occurences\"] += 1\n",
    "del seen\n",
    "rcdf = pd.DataFrame.from_dict(occurences, orient=\"index\")\n",
    "del occurences\n",
    "rcdf[\"train_precision\"] = (\n",
    "    rcdf[rcdf[\"train_total\"] > 0][\"train_occurences\"]\n",
    "    / rcdf[rcdf[\"train_total\"] > 0][\"train_total\"]\n",
    ")\n",
    "\n",
    "rcdf[\"val_precision\"] = (\n",
    "    rcdf[rcdf[[\"train_total\", \"val_total\"]].sum(axis=1) > 0][\"val_occurences\"]\n",
    "    / rcdf[rcdf[[\"train_total\", \"val_total\"]].sum(axis=1) > 0][\"val_total\"]\n",
    ")\n",
    "\n",
    "rcdf[\"test_precision\"] = (\n",
    "    rcdf[rcdf[[\"train_total\", \"val_total\", \"test_total\"]].sum(axis=1) > 0][\n",
    "        \"test_occurences\"\n",
    "    ]\n",
    "    / rcdf[rcdf[[\"train_total\", \"val_total\", \"test_total\"]].sum(axis=1) > 0][\n",
    "        \"test_total\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "rcdf[\"precision\"] = rcdf[[\"train_occurences\", \"val_occurences\", \"test_occurences\"]].sum(\n",
    "    axis=1\n",
    ") / np.where(\n",
    "    rcdf[\"train_total\"] > 0,\n",
    "    rcdf[\"train_total\"] + len(val_data) + len(test_data),\n",
    "    np.where(\n",
    "        rcdf[\"val_total\"] > 0, rcdf[\"val_total\"] + len(test_data), rcdf[\"test_total\"]\n",
    "    ),\n",
    ")\n",
    "display(rcdf)\n",
    "display(\n",
    "    rcdf[[\"train_precision\", \"val_precision\", \"test_precision\", \"precision\"]]\n",
    "    .describe()\n",
    "    .T.drop(columns=[\"count\"])\n",
    "    .style.format(\"{:.2%}\")\n",
    ")\n",
    "for x in (\"train_precision\", \"val_precision\", \"test_precision\", \"precision\"):\n",
    "    print(f\"{x:>15s}: \", end=\"\")\n",
    "    x = rcdf[x]\n",
    "    print(\n",
    "        f\"μ = {x.mean():.2%}, σ = {x.std():.2%}, median = {x.median():.2%}, mad = {(x.median() - x).abs().median():.2%}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fcd85f1-431f-4cfd-897e-33dab11e7c62",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in tqdm.tqdm(df.itertuples(), total=len(df)):\n",
    "    d = {(False, False): 0, (False, True): 0, (True, False): 0, (True, True): 0}\n",
    "    for s in train_data:\n",
    "        d[(i.Index[0] <= s, i.Index[1] <= s)] += 1\n",
    "    df.loc[i.Index, \"TN (train)\"] = d[(False, False)]\n",
    "    df.loc[i.Index, \"FP (train)\"] = d[(False, True)]\n",
    "    df.loc[i.Index, \"FN (train)\"] = d[(True, False)]\n",
    "    df.loc[i.Index, \"TP (train)\"] = d[(True, True)]\n",
    "\n",
    "df[[\"TP (train)\", \"FP (train)\", \"TN (train)\", \"FN (train)\"]] = df[\n",
    "    [\"TP (train)\", \"FP (train)\", \"TN (train)\", \"FN (train)\"]\n",
    "].astype(int)\n",
    "df[\"Recall (train)\"] = df[\"TP (train)\"] / df[[\"TP (train)\", \"FN (train)\"]].sum(axis=1)\n",
    "df[\"F1 (train)\"] = (\n",
    "    2\n",
    "    * df[[\"Confidence\", \"Recall (train)\"]].product(axis=1)\n",
    "    / df[[\"Confidence\", \"Recall (train)\"]].sum(axis=1)\n",
    ")\n",
    "df[\"Precision Random (train)\"] = df[[\"TP (train)\", \"FN (train)\"]].sum(axis=1) / df[\n",
    "    [\"TP (train)\", \"FP (train)\", \"TN (train)\", \"FN (train)\"]\n",
    "].sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec4e8b9-046d-4128-af4a-3dfc07be3c65",
   "metadata": {},
   "source": [
    "# Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f30999-2257-49c3-945c-6b8a2bef97cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in tqdm.tqdm(df.itertuples(), total=len(df)):\n",
    "    d = {(False, False): 0, (False, True): 0, (True, False): 0, (True, True): 0}\n",
    "    for s in val_data:\n",
    "        d[(i.Index[0] <= s, i.Index[1] <= s)] += 1\n",
    "    df.loc[i.Index, \"TN (val)\"] = d[(False, False)]\n",
    "    df.loc[i.Index, \"FP (val)\"] = d[(False, True)]\n",
    "    df.loc[i.Index, \"FN (val)\"] = d[(True, False)]\n",
    "    df.loc[i.Index, \"TP (val)\"] = d[(True, True)]\n",
    "df[[\"TP (val)\", \"FP (val)\", \"TN (val)\", \"FN (val)\"]] = df[\n",
    "    [\"TP (val)\", \"FP (val)\", \"TN (val)\", \"FN (val)\"]\n",
    "].astype(int)\n",
    "df[\"Precision (val)\"] = df[\"TP (val)\"] / (df[\"TP (val)\"] + df[\"FP (val)\"])\n",
    "df[\"Recall (val)\"] = df[\"TP (val)\"] / (df[\"TP (val)\"] + df[\"FN (val)\"])\n",
    "df[\"F1 (val)\"] = (\n",
    "    2\n",
    "    * (df[\"Precision (val)\"] * df[\"Recall (val)\"])\n",
    "    / (df[\"Precision (val)\"] + df[\"Recall (val)\"])\n",
    ")\n",
    "df[\"Accuracy (val)\"] = (df[\"TP (val)\"] + df[\"TN (val)\"]) / df[\n",
    "    [\"TP (val)\", \"FP (val)\", \"TN (val)\", \"FN (val)\"]\n",
    "].sum(axis=1)\n",
    "df[\"FPR (val)\"] = df[\"FP (val)\"] / df[[\"FP (val)\", \"TN (val)\"]].sum(axis=1)\n",
    "df[\"Precision Random (val)\"] = (df[\"TP (val)\"] + df[\"FN (val)\"]) / df[\n",
    "    [\"TP (val)\", \"FP (val)\", \"TN (val)\", \"FN (val)\"]\n",
    "].sum(axis=1)\n",
    "\n",
    "df.sort_values(\n",
    "    [\"Precision (val)\", \"F1 (val)\", \"Recall (val)\", \"Accuracy (val)\"], ascending=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d154d0-593f-4f43-a476-4371b5afca94",
   "metadata": {},
   "source": [
    "# Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b128e5d-6773-4a0f-9c8d-8ac997438717",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in tqdm.tqdm(df.itertuples(), total=len(df)):\n",
    "    d = {(False, False): 0, (False, True): 0, (True, False): 0, (True, True): 0}\n",
    "    for s in test_data:\n",
    "        d[(i.Index[0] <= s, i.Index[1] <= s)] += 1\n",
    "    df.loc[i.Index, \"TN\"] = d[(False, False)]\n",
    "    df.loc[i.Index, \"FP\"] = d[(False, True)]\n",
    "    df.loc[i.Index, \"FN\"] = d[(True, False)]\n",
    "    df.loc[i.Index, \"TP\"] = d[(True, True)]\n",
    "df[[\"TP\", \"FP\", \"TN\", \"FN\"]] = df[[\"TP\", \"FP\", \"TN\", \"FN\"]].astype(int)\n",
    "df[\"Precision\"] = df[\"TP\"] / (df[\"TP\"] + df[\"FP\"])\n",
    "df[\"Recall\"] = df[\"TP\"] / (df[\"TP\"] + df[\"FN\"])\n",
    "df[\"F1\"] = 2 * (df[\"Precision\"] * df[\"Recall\"]) / (df[\"Precision\"] + df[\"Recall\"])\n",
    "df[\"Accuracy\"] = (df[\"TP\"] + df[\"TN\"]) / df[[\"TP\", \"FP\", \"TN\", \"FN\"]].sum(axis=1)\n",
    "df[\"FPR\"] = df[\"FP\"] / df[[\"FP\", \"TN\"]].sum(axis=1)\n",
    "df[\"Precision Random\"] = (df[\"TP\"] + df[\"FN\"]) / df[[\"TP\", \"FP\", \"TN\", \"FN\"]].sum(\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "df.sort_values([\"Precision\", \"F1\", \"Recall\", \"Accuracy\"], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ef00da-e758-47d0-a56e-82e72f5dd4ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_axis(x, y, xrange=(0, 1), yrange=(0, 1), oneone=True, linreg=True):\n",
    "    fig = plt.figure(figsize=(12.5 * cm, 12.5 * cm), dpi=100)\n",
    "    ax = plt.subplot(111)\n",
    "    ax.xaxis.set_major_formatter(tck.PercentFormatter(xmax=1))\n",
    "    ax.yaxis.set_major_formatter(tck.PercentFormatter(xmax=1))\n",
    "    ax.set_xlim(*xrange)\n",
    "    ax.set_ylim(*yrange)\n",
    "    sns.despine(ax=ax)\n",
    "    sns.histplot(x=x, y=y, data=df, ax=ax)\n",
    "    if oneone:\n",
    "        plt.plot(xrange, xrange, linestyle=\":\", color=\"grey\")\n",
    "    if linreg:\n",
    "        sns.regplot(x=x, y=y, data=df, scatter=False, ci=None, truncate=False, ax=ax)\n",
    "        regdata = df[[x, y]].dropna()\n",
    "        regdata = linregress(regdata[x], regdata[y])\n",
    "        a, b, r2 = regdata.intercept, regdata.slope, regdata.rvalue ** 2\n",
    "        ax.add_artist(\n",
    "            AnchoredText(\n",
    "                f\"$f(x)={a:.2f}+{b:.2f}x$\\n$R^2 = {r2:.2f}$\",\n",
    "                frameon=False,\n",
    "                loc=\"lower right\",\n",
    "            )\n",
    "        )\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_axis(x=\"Precision Random\", y=\"Precision\")\n",
    "plot_axis(x=\"Precision Random (val)\", y=\"Precision (val)\")\n",
    "plot_axis(x=\"Precision Random (val)\", y=\"Precision Random\")\n",
    "\n",
    "plot_axis(\n",
    "    x=\"Confidence\",\n",
    "    y=\"Precision (val)\",\n",
    "    xrange=(df[\"Confidence\"].min(), 1),\n",
    "    oneone=False,\n",
    "    linreg=False,\n",
    ")\n",
    "plot_axis(\n",
    "    x=\"Confidence\",\n",
    "    y=\"Precision\",\n",
    "    xrange=(df[\"Confidence\"].min(), 1),\n",
    "    oneone=False,\n",
    "    linreg=False,\n",
    ")\n",
    "plot_axis(x=\"Precision (val)\", y=\"Precision\")\n",
    "del plot_axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d479cd6-e98d-446b-973f-8f5204ad6e4f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12.5 * cm, 12.5 * cm), dpi=144)\n",
    "ax = plt.subplot(111)\n",
    "data_ = pd.DataFrame(\n",
    "    data=(\n",
    "        (\n",
    "            p,\n",
    "            len(df[df[\"Precision (val)\"] > p][\"Precision\"]) / len(df),\n",
    "            df[df[\"Precision (val)\"] > p][\"Precision\"].mean(),\n",
    "            df[df[\"Precision (val)\"] > p][\"Precision\"].median(),\n",
    "            df[df[\"Precision (val)\"] > p][\"Precision\"].quantile(0.05),\n",
    "        )\n",
    "        for p in np.linspace(0, 1, 101)\n",
    "    ),\n",
    "    columns=[\n",
    "        \"Threshold Precision (val)\",\n",
    "        \"% Rules left\",\n",
    "        \"Mean Precision\",\n",
    "        \"Median Precision\",\n",
    "        \"5th Percentile Precision\",\n",
    "    ],\n",
    ")\n",
    "sns.lineplot(\n",
    "    x=\"Threshold Precision (val)\",\n",
    "    y=\"Value\",\n",
    "    hue=\"Type\",\n",
    "    data=data_.melt(\n",
    "        id_vars=[\"Threshold Precision (val)\"], var_name=\"Type\", value_name=\"Value\"\n",
    "    ),\n",
    "    ax=ax,\n",
    ")\n",
    "ax.xaxis.set_major_formatter(tck.PercentFormatter(xmax=1))\n",
    "ax.yaxis.set_major_formatter(tck.PercentFormatter(xmax=1))\n",
    "ax.set_xlim(0, 1)\n",
    "ax.set_ylim(0, 1)\n",
    "plt.plot([0.95, 0.95], [0, 1], linestyle=\":\", color=\"grey\")\n",
    "sns.despine(ax=ax)\n",
    "sns.move_legend(\n",
    "    ax,\n",
    "    \"center left\",\n",
    "    bbox_to_anchor=(1, 0.5),\n",
    "    ncol=1,\n",
    "    frameon=False,\n",
    ")\n",
    "plt.show()\n",
    "display(data_.tail(25))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fceb6bb-2cb2-4b9e-a15e-5d55539f0a60",
   "metadata": {},
   "source": [
    "# Activity Change between train and val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7afb209-32e5-4d77-abdc-9e893ed9235f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df[\"Activity Change\"] = (\n",
    "    df[\"Precision Random (val)\"] / df[\"Precision Random (train)\"] - 1\n",
    ")\n",
    "fig = plt.figure(figsize=(12.5 * cm, 12.5 * cm), dpi=100)\n",
    "ax = plt.subplot(111)\n",
    "df[\"Activity Change\"].hist(bins=15, ax=ax)\n",
    "ax.xaxis.set_major_formatter(tck.PercentFormatter(xmax=1))\n",
    "sns.despine(ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c33957-3058-423e-bbd5-d773ba33046f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Sample: Precision Advantage over Random in train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754f7548-2581-4926-98db-9a279234205e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tdf = pd.DataFrame(\n",
    "    (\n",
    "        (\n",
    "            tuple(groups.keys())[len(train_data) + j],\n",
    "            i.Index[0],\n",
    "            i.Index[1],\n",
    "            i.Index[0] <= s,\n",
    "            i.Index[1] <= s,\n",
    "        )\n",
    "        for i in df[(df[\"Precision (val)\"] - df[\"Precision Random (val)\"]) > 0]\n",
    "        .sample(5)\n",
    "        .itertuples()\n",
    "        for j, s in enumerate(val_data)\n",
    "    ),\n",
    "    columns=[\"Date\", \"RHS\", \"LHS\", \"RHS?\", \"LHS?\"],\n",
    ")\n",
    "tdf[\"Date\"] = pd.to_datetime(tdf[\"Date\"])\n",
    "tdf = (\n",
    "    tdf.set_index(\"Date\")\n",
    "    .groupby([\"RHS\", \"LHS\", pd.Grouper(freq=\"D\")])\n",
    "    .sum()\n",
    "    .astype(bool)\n",
    "    .sort_index()\n",
    ")\n",
    "tdf[\"TP\"] = tdf[\"LHS?\"] & tdf[\"RHS?\"]\n",
    "tdf[\"TN\"] = ~tdf[\"LHS?\"] & ~tdf[\"RHS?\"]\n",
    "tdf[\"FP\"] = tdf[\"LHS?\"] & ~tdf[\"RHS?\"]\n",
    "tdf[\"FN\"] = ~tdf[\"LHS?\"] & tdf[\"RHS?\"]\n",
    "\n",
    "tdf = (\n",
    "    tdf.drop(columns=[\"LHS?\", \"RHS?\"])\n",
    "    .reset_index(level=[0, 1])\n",
    "    .groupby([\"RHS\", \"LHS\"])\n",
    "    .expanding()\n",
    "    .sum()\n",
    "    .astype(int)\n",
    ")\n",
    "\n",
    "tdf[\"Precision\"] = tdf[\"TP\"] / tdf[[\"TP\", \"FP\"]].sum(axis=1)\n",
    "tdf[\"Precision Random\"] = tdf[[\"TP\", \"FN\"]].sum(axis=1) / tdf[\n",
    "    [\"TP\", \"FP\", \"TN\", \"FN\"]\n",
    "].sum(axis=1)\n",
    "\n",
    "tdf[\"Precision - Precision Random\"] = tdf[\"Precision\"] - tdf[\"Precision Random\"]\n",
    "\n",
    "tdf.reset_index(inplace=True)\n",
    "tdf[\"Rule\"] = (\n",
    "    tdf[\"LHS\"].apply(lambda l: \", \".join(str(it) for it in sorted(l)))\n",
    "    + \" -> \"\n",
    "    + tdf[\"RHS\"].apply(lambda l: \", \".join(str(it) for it in sorted(l)))\n",
    ")\n",
    "\n",
    "fig = plt.figure(figsize=(40 * cm, 30 * cm), dpi=100)\n",
    "ax = plt.subplot(111)\n",
    "sns.lineplot(x=\"Date\", y=\"Precision - Precision Random\", hue=\"Rule\", data=tdf, ax=ax)\n",
    "ax.set_ylim(-1, 1)\n",
    "ax.yaxis.set_major_formatter(tck.PercentFormatter(xmax=1))\n",
    "sns.move_legend(\n",
    "    ax,\n",
    "    \"center left\",\n",
    "    bbox_to_anchor=(1, 0.5),\n",
    "    ncol=1,\n",
    "    frameon=False,\n",
    ")\n",
    "sns.despine(ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3a4d62-6ae3-426d-8e8c-bbfc2f4b3d71",
   "metadata": {},
   "source": [
    "## Final Rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd0fcea-9245-48c2-8d4f-c01a6a45ac68",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "t_val = 0.95\n",
    "final_rules = df[df[\"Precision (val)\"] > t_val]\n",
    "display(final_rules[[\"Precision\", \"Recall\", \"F1\", \"Accuracy\"]])\n",
    "display(final_rules[[\"Precision\", \"Recall\", \"F1\", \"Accuracy\"]].describe().T)\n",
    "\n",
    "final_rules[[\"Precision\", \"Recall\", \"F1\", \"Accuracy\"]].hist(figsize=(10, 5))\n",
    "plt.suptitle(\n",
    "    \"Association Rules results histogram on test set with Val Precision > 0.95\"\n",
    ")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ebb760-f35d-47da-a8c3-6235b2cd78ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "exported_rules = (\n",
    "    final_rules.reset_index().groupby(\"RHS\")[\"LHS\"].apply(list).reset_index()\n",
    ")\n",
    "exported_rules.to_pickle(\"rules_dict.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1abc09-05bc-4812-baea-a864e1865f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "exported_rules_eval = []\n",
    "for date, group in tqdm.tqdm(groups.items(), total=len(groups)):\n",
    "    for rule in exported_rules.itertuples():\n",
    "        exported_rules_eval.append(\n",
    "            (date, rule.RHS, tuple(lhs <= set(group) for lhs in rule.LHS))\n",
    "        )\n",
    "pd.DataFrame(exported_rules_eval, columns=[\"Date\", \"RHS\", \"LHSs\"]).set_index(\n",
    "    [\"RHS\", \"Date\"]\n",
    ").sort_index().to_csv(\"rules_active.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ae2eca-4f6c-4c55-8285-06547bcc7917",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6a89ca-9332-4b02-adee-d310bdf73a4c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@ipywidgets.interact(\n",
    "    x=ipywidgets.ToggleButtons(options=[\"Confidence\", \"Support\", \"Lift\"]),\n",
    "    y=ipywidgets.ToggleButtons(options=[\"Precision\", \"Recall\", \"F1\", \"Accuracy\"]),\n",
    "    support=ipywidgets.FloatRangeSlider(\n",
    "        value=(0.0, 1.0),\n",
    "        min=0.0,\n",
    "        max=1.0,\n",
    "        step=0.001,\n",
    "        readout_format=\".1%\",\n",
    "        continuous_update=False,\n",
    "    ),\n",
    "    confidence=ipywidgets.FloatRangeSlider(\n",
    "        value=(0.0, 1.0),\n",
    "        min=0.0,\n",
    "        max=1.0,\n",
    "        step=0.001,\n",
    "        readout_format=\".1%\",\n",
    "        continuous_update=False,\n",
    "    ),\n",
    "    q=ipywidgets.IntSlider(value=4, min=1, max=10, step=1, continuous_update=False),\n",
    ")\n",
    "def plot_correlation(x, y, support, confidence, q):\n",
    "    temp = df[\n",
    "        df[\"Support\"].between(*support) & df[\"Confidence\"].between(*confidence)\n",
    "    ].copy()\n",
    "    g = sns.jointplot(\n",
    "        x=x,\n",
    "        y=y,\n",
    "        data=temp,\n",
    "        kind=\"hist\",\n",
    "        height=20 * cm,\n",
    "    )\n",
    "    g.ax_joint.set_xlim(temp[x].min(), temp[x].max())\n",
    "    if x in {\"Confidence\", \"Support\"}:\n",
    "        g.ax_joint.xaxis.set_major_formatter(tck.PercentFormatter(xmax=1))\n",
    "    g.ax_joint.set_ylim(0, 1)\n",
    "    g.ax_joint.yaxis.set_major_formatter(tck.PercentFormatter(xmax=1))\n",
    "    plt.show()\n",
    "    temp[f\"{y} Interval\"] = pd.cut(\n",
    "        temp[y],\n",
    "        q,\n",
    "        labels=[f\"≤ {x[1]:.0%}\" for x in pd.interval_range(0, 1, q).to_tuples()],\n",
    "    )\n",
    "    g = sns.jointplot(\n",
    "        x=x,\n",
    "        y=y,\n",
    "        hue=f\"{y} Interval\",\n",
    "        palette=sns.color_palette(\"mako\", n_colors=q),\n",
    "        data=temp,\n",
    "        kind=\"scatter\",\n",
    "        height=20 * cm,\n",
    "        marginal_kws={\"common_norm\": False},\n",
    "    )\n",
    "    g.ax_joint.set_xlim(temp[x].min(), temp[x].max())\n",
    "    if x in {\"Confidence\", \"Support\"}:\n",
    "        g.ax_joint.xaxis.set_major_formatter(tck.PercentFormatter(xmax=1))\n",
    "    g.ax_joint.set_ylim(0, 1)\n",
    "    g.ax_joint.yaxis.set_major_formatter(tck.PercentFormatter(xmax=1))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f17e3b9-2543-4920-9afe-b6303407dfbd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@ipywidgets.interact(\n",
    "    x=ipywidgets.ToggleButtons(options=[\"Confidence\", \"Support\", \"Lift\"]),\n",
    "    y=ipywidgets.ToggleButtons(options=[\"Confidence\", \"Support\", \"Lift\"]),\n",
    ")\n",
    "def plot_correlation(x, y):\n",
    "    temp = df.copy()\n",
    "    g = sns.jointplot(\n",
    "        x=x,\n",
    "        y=y,\n",
    "        data=temp,\n",
    "        kind=\"hist\",\n",
    "        height=20 * cm,\n",
    "    )\n",
    "    if x in {\"Confidence\", \"Support\"}:\n",
    "        g.ax_joint.xaxis.set_major_formatter(tck.PercentFormatter(xmax=1))\n",
    "    if y in {\"Confidence\", \"Support\"}:\n",
    "        g.ax_joint.yaxis.set_major_formatter(tck.PercentFormatter(xmax=1))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c1c4c7-4839-497f-ba82-22dcccf142da",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Evaluation per Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98dea53b-3b4d-4080-a465-61b6bb9014ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Not used, keep for future if we don't care weighing rules\n",
    "\"\"\"\n",
    "df2 = (\n",
    "    df.reset_index()[[\"LHS\", \"RHS\"]]\n",
    "    .apply(lambda x: pd.Series([x[\"LHS\"]] + [(rhs,) for rhs in x[\"RHS\"]]), axis=1)\n",
    "    .melt(id_vars=[0])\n",
    "    .dropna()[[0, \"value\"]]\n",
    "    .rename(columns={0: \"LHS\", \"value\": \"RHS\"})\n",
    ")\n",
    "df2[\"RHS\"] = df2[\"RHS\"].apply(lambda t: t[0])\n",
    "df2 = (\n",
    "    df2.groupby(\"RHS\")[\"LHS\"]\n",
    "    .apply(lambda x: frozenset(x))\n",
    "    .reset_index()\n",
    "    .set_index(\"RHS\")\n",
    ")\n",
    "df2\n",
    "\"\"\"\n",
    "\n",
    "df2 = (\n",
    "    df[df[\"Precision (val)\"] > t_val]\n",
    "    .reset_index()[[\"LHS\", \"RHS\", \"Lift\"]]\n",
    "    .apply(\n",
    "        lambda x: pd.Series([x[\"LHS\"], x[\"Lift\"]] + [(rhs,) for rhs in x[\"RHS\"]]),\n",
    "        axis=1,\n",
    "    )\n",
    "    .melt(id_vars=[0, 1])\n",
    "    .dropna()\n",
    "    .drop(columns=[\"variable\"])\n",
    "    .rename(columns={0: \"LHS\", 1: \"Lift\", \"value\": \"RHS\"})\n",
    ")\n",
    "df2[\"RHS\"] = df2[\"RHS\"].apply(lambda t: t[0])\n",
    "df2 = (\n",
    "    df2.groupby(\"RHS\")[[\"LHS\", \"Lift\"]]\n",
    "    .apply(lambda x, axis: dict(zip(x[\"LHS\"], x[\"Lift\"])), axis=1)\n",
    "    .reset_index()\n",
    "    .set_index(\"RHS\")\n",
    "    .rename(columns={0: \"LHS\"})\n",
    ")\n",
    "\n",
    "for i in tqdm.tqdm(df2.itertuples(index=True), total=len(df2)):\n",
    "    d = {(False, False): 0, (False, True): 0, (True, False): 0, (True, True): 0}\n",
    "    for s in test_data:\n",
    "        t = [(lhs <= s, lift) for lhs, lift in i.LHS.items()]\n",
    "        total = sum(lift for _, lift in t)\n",
    "        signal = sum(lift for sig, lift in t if sig)\n",
    "        d[(i.Index in s, (signal / total) > 0.8)] += 1\n",
    "    df2.loc[i.Index, \"TN\"] = d[(False, False)]\n",
    "    df2.loc[i.Index, \"FP\"] = d[(False, True)]\n",
    "    df2.loc[i.Index, \"FN\"] = d[(True, False)]\n",
    "    df2.loc[i.Index, \"TP\"] = d[(True, True)]\n",
    "df2[[\"TP\", \"FP\", \"TN\", \"FN\"]] = df2[[\"TP\", \"FP\", \"TN\", \"FN\"]].astype(int)\n",
    "df2[\"Precision\"] = df2[\"TP\"] / (df2[\"TP\"] + df2[\"FP\"])\n",
    "df2[\"Recall\"] = df2[\"TP\"] / (df2[\"TP\"] + df2[\"FN\"])\n",
    "df2[\"F1\"] = 2 * (df2[\"Precision\"] * df2[\"Recall\"]) / (df2[\"Precision\"] + df2[\"Recall\"])\n",
    "df2[\"Accuracy\"] = (df2[\"TP\"] + df2[\"TN\"]) / df2[[\"TP\", \"FP\", \"TN\", \"FN\"]].sum(axis=1)\n",
    "\n",
    "df2[\"Precision Random\"] = df2[[\"TP\", \"FN\"]].sum(axis=1) / df2[\n",
    "    [\"TP\", \"FP\", \"TN\", \"FN\"]\n",
    "].sum(axis=1)\n",
    "\n",
    "display(df2.sort_values([\"Precision\", \"F1\", \"Recall\", \"Accuracy\"], ascending=False))\n",
    "\n",
    "prec = df2[\"TP\"].sum() / df2[[\"TP\", \"FP\"]].sum().sum()\n",
    "rec = df2[\"TP\"].sum() / df2[[\"TP\", \"FN\"]].sum().sum()\n",
    "acc = df2[[\"TP\", \"TN\"]].sum().sum() / df2[[\"TP\", \"FP\", \"TN\", \"FN\"]].sum().sum()\n",
    "f1 = 2 * prec * rec / (prec + rec)\n",
    "\n",
    "df2[[\"Precision\", \"Recall\", \"F1\", \"Accuracy\"]].describe().T"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
