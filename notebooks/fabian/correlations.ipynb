{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4da73e-e3eb-4a2c-a06b-28f566f49493",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext lab_black\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import ticker as tck\n",
    "import seaborn as sns\n",
    "import ipywidgets\n",
    "import pydantic\n",
    "import typing\n",
    "import datetime\n",
    "import itertools\n",
    "import collections\n",
    "import json\n",
    "import gzip\n",
    "import tqdm\n",
    "import multiprocessing\n",
    "from efficient_apriori import apriori\n",
    "import math\n",
    "import locale\n",
    "\n",
    "locale.setlocale(locale.LC_ALL, \"de_DE\")\n",
    "locale._override_localeconv[\"thousands_sep\"] = \".\"\n",
    "locale._override_localeconv[\"grouping\"] = [3, 3, 0]\n",
    "plt.rcParams[\"axes.formatter.use_locale\"] = True\n",
    "sns.set_theme(style=\"ticks\")\n",
    "cm = 1 / 2.54\n",
    "a4 = 29.7, 42\n",
    "\n",
    "\n",
    "class InfoboxProperty(pydantic.BaseModel):\n",
    "    propertyType: typing.Optional[str]\n",
    "    name: str\n",
    "\n",
    "\n",
    "class InfoboxChange(pydantic.BaseModel):\n",
    "    property: InfoboxProperty\n",
    "    valueValidTo: typing.Optional[datetime.datetime] = None\n",
    "    currentValue: typing.Optional[str] = None\n",
    "    previousValue: typing.Optional[str] = None\n",
    "\n",
    "\n",
    "class User(pydantic.BaseModel):\n",
    "    username: typing.Optional[str]\n",
    "    id: typing.Optional[int]\n",
    "\n",
    "\n",
    "class InfoboxRevision(pydantic.BaseModel):\n",
    "    revisionId: int\n",
    "    pageTitle: str\n",
    "    changes: typing.Sequence[InfoboxChange]\n",
    "    validFrom: datetime.datetime\n",
    "    attributes: typing.Optional[typing.Dict[str, str]]\n",
    "    pageID: int\n",
    "    revisionType: typing.Optional[str]\n",
    "    key: str\n",
    "    template: typing.Optional[str] = None\n",
    "    position: typing.Optional[int] = None\n",
    "    user: typing.Optional[User] = None\n",
    "    validTo: typing.Optional[datetime.datetime] = None\n",
    "\n",
    "\n",
    "class ChangeBuckets(pydantic.BaseModel):\n",
    "    filename: str\n",
    "    changes: typing.Dict[str, typing.Sequence[typing.Hashable]]\n",
    "\n",
    "\n",
    "def sliding(seq, window_size):\n",
    "    for i in range(len(seq) - window_size + 1):\n",
    "        yield seq[i : i + window_size]\n",
    "\n",
    "\n",
    "def overlapping_groups(groups, window_size):\n",
    "    return {\n",
    "        keys[0]: set().union(*(groups[key] for key in keys))\n",
    "        for keys in sliding(tuple(groups.keys()), window_size)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dafdd18-b18e-4726-a5f2-83eaf05cf7b3",
   "metadata": {},
   "source": [
    "# Creating Buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857cb9f0-c60c-4130-bdc2-98ad5723f785",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_pageid(file):\n",
    "    groups = collections.defaultdict(set)\n",
    "    with open(file) as f:\n",
    "        revisions = (InfoboxRevision.parse_raw(line) for line in f)\n",
    "        for revision in revisions:\n",
    "            groups[revision.validFrom.date().isoformat()].add(revision.pageID)\n",
    "    return ChangeBuckets(\n",
    "        filename=file.name,\n",
    "        changes={k: tuple(sorted(groups[k])) for k in sorted(groups.keys())},\n",
    "    )\n",
    "\n",
    "\n",
    "def process_property(file):\n",
    "    groups = collections.defaultdict(set)\n",
    "    with open(file) as f:\n",
    "        revisions = (InfoboxRevision.parse_raw(line) for line in f)\n",
    "        for revision in revisions:\n",
    "            groups[revision.validFrom.date().isoformat()].update(\n",
    "                change.property.name for change in revision.changes\n",
    "            )\n",
    "    return ChangeBuckets(\n",
    "        filename=file.name,\n",
    "        changes={k: tuple(sorted(groups[k])) for k in sorted(groups.keys())},\n",
    "    )\n",
    "\n",
    "\n",
    "def process_template_property(file):\n",
    "    groups = collections.defaultdict(set)\n",
    "    with open(file) as f:\n",
    "        revisions = (InfoboxRevision.parse_raw(line) for line in f)\n",
    "        for revision in revisions:\n",
    "            groups[revision.validFrom.date().isoformat()].update(\n",
    "                (str(revision.template), change.property.name)\n",
    "                for change in revision.changes\n",
    "            )\n",
    "    return ChangeBuckets(\n",
    "        filename=file.name,\n",
    "        changes={k: tuple(sorted(groups[k])) for k in sorted(groups.keys())},\n",
    "    )\n",
    "\n",
    "\n",
    "def process_page_property(file):\n",
    "    groups = collections.defaultdict(set)\n",
    "    with open(file) as f:\n",
    "        revisions = (InfoboxRevision.parse_raw(line) for line in f)\n",
    "        for revision in revisions:\n",
    "            groups[revision.validFrom.date().isoformat()].update(\n",
    "                (revision.pageID, change.property.name) for change in revision.changes\n",
    "            )\n",
    "    return ChangeBuckets(\n",
    "        filename=file.name,\n",
    "        changes={k: tuple(sorted(groups[k])) for k in sorted(groups.keys())},\n",
    "    )\n",
    "\n",
    "\n",
    "fname = \"./changesets-pageid.json.gz\"\n",
    "\n",
    "if not Path(fname).exists():\n",
    "    groups = collections.defaultdict(set)\n",
    "    files = [\n",
    "        x\n",
    "        for x in sorted(\n",
    "            Path(\"../../matched-infoboxes-extracted/\").rglob(\"*.output.json\")\n",
    "        )\n",
    "        if x.is_file()\n",
    "    ]\n",
    "    with multiprocessing.Pool(4) as p:\n",
    "        imap = p.imap(process_page_property, files)\n",
    "        for cb in tqdm.tqdm(imap, total=len(files)):\n",
    "            for k, v in cb.changes.items():\n",
    "                groups[k].update(v)\n",
    "    del files\n",
    "    groups = {k: tuple(sorted(groups[k])) for k in sorted(groups.keys())}\n",
    "    with open(fname, \"wb\") as f:\n",
    "        f.write(\n",
    "            gzip.compress(\n",
    "                ChangeBuckets(filename=\"all\", changes=groups)\n",
    "                .json(indent=None, separators=(\",\", \":\"))\n",
    "                .encode(\"utf-8\")\n",
    "            )\n",
    "        )\n",
    "else:\n",
    "    with open(fname, \"rb\") as f:\n",
    "        groups = ChangeBuckets.parse_raw(\n",
    "            gzip.decompress(f.read()).decode(\"utf-8\")\n",
    "        ).changes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f517a8-15ed-4290-98c9-d9fec522c864",
   "metadata": {},
   "source": [
    "# Apriori Association Rule Mining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ebca635-7318-45cf-b15e-f3c1b84d84b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_days = 1\n",
    "data = tuple(overlapping_groups(groups, n_days).values())\n",
    "test_size = math.ceil(len(data) * 0.2)\n",
    "train_data = data[: len(data) - test_size]\n",
    "test_data = data[len(data) - test_size :]\n",
    "del data, test_size\n",
    "itemsets, rules = apriori(\n",
    "    train_data,\n",
    "    min_support=0.005 * n_days,\n",
    "    min_confidence=0.8,\n",
    "    max_length=2,\n",
    ")\n",
    "del n_days\n",
    "\n",
    "df = (\n",
    "    pd.DataFrame(\n",
    "        [\n",
    "            (\n",
    "                rule.lhs[0],\n",
    "                rule.rhs[0],\n",
    "                rule.confidence,\n",
    "                rule.support,\n",
    "                rule.lift,\n",
    "                rule.conviction,\n",
    "            )\n",
    "            for rule in rules\n",
    "        ],\n",
    "        columns=[\"LHS\", \"RHS\", \"Confidence\", \"Support\", \"Lift\", \"Conviction\"],\n",
    "    )\n",
    "    .set_index([\"LHS\", \"RHS\"])\n",
    "    .sort_index()\n",
    ")\n",
    "display(df.describe().T.style.format(\"{:.2f}\"))\n",
    "display(df.sort_values(\"Lift\", ascending=False).head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d154d0-593f-4f43-a476-4371b5afca94",
   "metadata": {},
   "source": [
    "# Evaluation per Association Rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b128e5d-6773-4a0f-9c8d-8ac997438717",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in df.itertuples():\n",
    "    d = {(False, False): 0, (False, True): 0, (True, False): 0, (True, True): 0}\n",
    "    for s in test_data:\n",
    "        d[(i.Index[1] in s, i.Index[0] in s)] += 1\n",
    "    df.loc[i.Index, \"TN\"] = d[(False, False)]\n",
    "    df.loc[i.Index, \"FP\"] = d[(False, True)]\n",
    "    df.loc[i.Index, \"FN\"] = d[(True, False)]\n",
    "    df.loc[i.Index, \"TP\"] = d[(True, True)]\n",
    "df[[\"TP\", \"FP\", \"TN\", \"FN\"]] = df[[\"TP\", \"FP\", \"TN\", \"FN\"]].astype(int)\n",
    "df[\"Precision\"] = (df[\"TP\"] / (df[\"TP\"] + df[\"FP\"])).fillna(0)\n",
    "df[\"Recall\"] = (df[\"TP\"] / (df[\"TP\"] + df[\"FN\"])).fillna(0)\n",
    "df[\"F1\"] = (\n",
    "    2 * (df[\"Precision\"] * df[\"Recall\"]) / (df[\"Precision\"] + df[\"Recall\"])\n",
    ").fillna(0)\n",
    "df[\"Accuracy\"] = (\n",
    "    (df[\"TP\"] + df[\"TN\"]) / df[[\"TP\", \"FP\", \"TN\", \"FN\"]].sum(axis=1)\n",
    ").fillna(0)\n",
    "\n",
    "df.sort_values([\"F1\", \"Precision\", \"Recall\", \"Accuracy\"], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c1c4c7-4839-497f-ba82-22dcccf142da",
   "metadata": {},
   "source": [
    "# Evaluation per Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3f019b-c698-4a5f-aa49-c8b96ef9f40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = (\n",
    "    df.reset_index()[[\"LHS\", \"RHS\"]]\n",
    "    .groupby(\"RHS\")[\"LHS\"]\n",
    "    .apply(lambda lhs: tuple(sorted(set(lhs))))\n",
    "    .reset_index()\n",
    "    .set_index(\"RHS\")\n",
    "    .sort_index()\n",
    ")\n",
    "\n",
    "for i in df2.itertuples(index=True):\n",
    "    d = {(False, False): 0, (False, True): 0, (True, False): 0, (True, True): 0}\n",
    "    for s in test_data:\n",
    "        d[(i.Index in s, any((j in s) for j in i.LHS))] += 1\n",
    "    df2.loc[i.Index, \"TN\"] = d[(False, False)]\n",
    "    df2.loc[i.Index, \"FP\"] = d[(False, True)]\n",
    "    df2.loc[i.Index, \"FN\"] = d[(True, False)]\n",
    "    df2.loc[i.Index, \"TP\"] = d[(True, True)]\n",
    "df2[[\"TP\", \"FP\", \"TN\", \"FN\"]] = df2[[\"TP\", \"FP\", \"TN\", \"FN\"]].astype(int)\n",
    "df2[\"Precision\"] = (df2[\"TP\"] / (df2[\"TP\"] + df2[\"FP\"])).fillna(0)\n",
    "df2[\"Recall\"] = (df2[\"TP\"] / (df2[\"TP\"] + df2[\"FN\"])).fillna(0)\n",
    "df2[\"F1\"] = (\n",
    "    2 * (df2[\"Precision\"] * df2[\"Recall\"]) / (df2[\"Precision\"] + df2[\"Recall\"])\n",
    ").fillna(0)\n",
    "df2[\"Accuracy\"] = (\n",
    "    (df2[\"TP\"] + df2[\"TN\"]) / df2[[\"TP\", \"FP\", \"TN\", \"FN\"]].sum(axis=1)\n",
    ").fillna(0)\n",
    "\n",
    "display(df2.sort_values([\"F1\", \"Precision\", \"Recall\", \"Accuracy\"], ascending=False))\n",
    "\n",
    "print(\n",
    "    \"Predictor totals:\",\n",
    "    f'Precision: {locale.format_string(\"%.2f%%\", 100*df2[\"TP\"].sum() / (df2[\"TP\"].sum() + df2[\"FP\"].sum()), True)}',\n",
    "    f'Recall: {locale.format_string(\"%.2f%%\", 100*df2[\"TP\"].sum() / (df2[\"TP\"].sum() + df2[\"FN\"].sum()), True)}',\n",
    "    f'F1: {locale.format_string(\"%.2f%%\", 100*2*(df2[\"TP\"].sum() / (df2[\"TP\"].sum() + df2[\"FP\"].sum())*df2[\"TP\"].sum() / (df2[\"TP\"].sum() + df2[\"FN\"].sum()))/(df2[\"TP\"].sum() / (df2[\"TP\"].sum() + df2[\"FP\"].sum())+df2[\"TP\"].sum() / (df2[\"TP\"].sum() + df2[\"FN\"].sum())), True)}',\n",
    "    f'Accuracy: {locale.format_string(\"%.2f%%\", 100*(df2[\"TP\"].sum() + df2[\"TN\"].sum()) / df2[[\"TP\", \"FP\", \"TN\", \"FN\"]].sum().sum(), True)}',\n",
    "    sep=\"\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332b7f8f-965c-4207-a885-96ac153ad63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "changes_predictable = (\n",
    "    df2[[\"TP\", \"FP\", \"TN\", \"FN\"]].sum().sum()\n",
    ")  # == len(df2) * len(test_data)\n",
    "total_changes = sum(len(i) for i in test_data)\n",
    "print(\n",
    "    f'Changes predictable: {locale.format_string(\"%d\", changes_predictable, True)}',\n",
    "    f'Changes happened: {locale.format_string(\"%d\", total_changes, True)}',\n",
    "    f'--> {locale.format_string(\"%.2f%%\", 100 * changes_predictable / total_changes, True)}',\n",
    "    sep=\"\\n\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
